{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a. Loading up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. loading process:\n",
    "** dictionary or pandas dataframe\n",
    "** divide into train and test set\n",
    "** divide train into train and validation set\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files in the directory:  ['.ipynb_checkpoints', 'data-ear-1.csv', 'data-ear-2.csv', 'data-ear-3.csv', 'data-hand-landscape-1.csv', 'data-hand-landscape-2.csv', 'data-hand-landscape-3.csv', 'data-hand-portrait-1.csv', 'data-hand-portrait-2.csv', 'data-hand-portrait-3.csv', 'data-hand-swinging-1.csv', 'data-hand-swinging-2.csv', 'data-hand-swinging-3.csv', 'data-pocket-1.csv', 'data-pocket-2.csv', 'data-pocket-3.csv', 'mapsted_rd_project.ipynb', 'test-dataset-1.csv', 'test-dataset-2.csv', 'Testcase5_RD_Engineer.pdf'] \n",
      "\n",
      "The csv files that are filtered:  ['data-ear-1.csv', 'data-ear-2.csv', 'data-ear-3.csv', 'data-hand-landscape-1.csv', 'data-hand-landscape-2.csv', 'data-hand-landscape-3.csv', 'data-hand-portrait-1.csv', 'data-hand-portrait-2.csv', 'data-hand-portrait-3.csv', 'data-hand-swinging-1.csv', 'data-hand-swinging-2.csv', 'data-hand-swinging-3.csv', 'data-pocket-1.csv', 'data-pocket-2.csv', 'data-pocket-3.csv', 'test-dataset-1.csv', 'test-dataset-2.csv']\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir()\n",
    "csv_files = []\n",
    "\n",
    "# take out only the csv files\n",
    "for f in all_files:\n",
    "    if '.csv' in f:\n",
    "        csv_files.append(f)\n",
    "        \n",
    "print(\"The files in the directory: \", all_files, \"\\n\")\n",
    "print(\"The csv files that are filtered: \", csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file - data-ear-1.csv - has been loaded!\n",
      "The file - data-ear-2.csv - has been loaded!\n",
      "The file - data-ear-3.csv - has been loaded!\n",
      "The file - data-hand-landscape-1.csv - has been loaded!\n",
      "The file - data-hand-landscape-2.csv - has been loaded!\n",
      "The file - data-hand-landscape-3.csv - has been loaded!\n",
      "The file - data-hand-portrait-1.csv - has been loaded!\n",
      "The file - data-hand-portrait-2.csv - has been loaded!\n",
      "The file - data-hand-portrait-3.csv - has been loaded!\n",
      "The file - data-hand-swinging-1.csv - has been loaded!\n",
      "The file - data-hand-swinging-2.csv - has been loaded!\n",
      "The file - data-hand-swinging-3.csv - has been loaded!\n",
      "The file - data-pocket-1.csv - has been loaded!\n",
      "The file - data-pocket-2.csv - has been loaded!\n",
      "The file - data-pocket-3.csv - has been loaded!\n",
      "The file - test-dataset-1.csv - has been loaded!\n",
      "The file - test-dataset-2.csv - has been loaded!\n"
     ]
    }
   ],
   "source": [
    "# function to load each dataset as a dictionary of pandas dataframes\n",
    "def load_csv_file(flist):\n",
    "    # flist: list of files to be loaded\n",
    "    \n",
    "    fdict = dict()\n",
    "    for f in flist:\n",
    "        csv_file = pd.read_csv(f)\n",
    "        fdict[f] = csv_file\n",
    "        print(\"The file -\", f, \"- has been loaded!\")\n",
    "    return fdict\n",
    "\n",
    "# load each csv file with the load_csv_file function\n",
    "csv_dict = load_csv_file(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data-ear-1.csv', 'data-ear-2.csv', 'data-ear-3.csv', 'data-hand-landscape-1.csv', 'data-hand-landscape-2.csv', 'data-hand-landscape-3.csv', 'data-hand-portrait-1.csv', 'data-hand-portrait-2.csv', 'data-hand-portrait-3.csv', 'data-hand-swinging-1.csv', 'data-hand-swinging-2.csv', 'data-hand-swinging-3.csv', 'data-pocket-1.csv', 'data-pocket-2.csv', 'data-pocket-3.csv', 'test-dataset-1.csv', 'test-dataset-2.csv'])\n"
     ]
    }
   ],
   "source": [
    "print(csv_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b. Examine the characteristics of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to examine each dataset when required\n",
    "def examine_data(pd_data):\n",
    "    # pd_data: should be a pandas loaded csv file!\n",
    "    \n",
    "    print(\"The features of the each dataset are as follows:\")\n",
    "    print(pd_data.columns.values)\n",
    "    print(\"################################################\\n\\n\")\n",
    "    \n",
    "    print(\"Shape of each dataset:\")\n",
    "    print(pd_data.shape)\n",
    "    print(\"################################################\\n\\n\")\n",
    "    \n",
    "    print(\"The first 10 rows of the file are: \")\n",
    "    print(pd_data.head(10))\n",
    "    print(\"################################################\\n\\n\")\n",
    "    \n",
    "    print(\"Correlation between features of the file: \")\n",
    "    print(pd_data.corr())\n",
    "    print(\"################################################\\n\\n\")\n",
    "    \n",
    "    print(\"Basic statistical analysis of the dataset: \")\n",
    "    print(pd_data.describe())\n",
    "    print(\"################################################\\n\\n\")\n",
    "    \n",
    "    print(\"Examine the heatmap of the correlations of the dataset: \")\n",
    "    plt.figure(figsize=(10, 5))      # Sample figsize in inches\n",
    "    sns.heatmap(pd_data.corr(), cmap=\"RdYlGn\")\n",
    "    plt.show()\n",
    "    print(\"################################################\\n\\n\")\n",
    "    \n",
    "    '''\n",
    "    print(\"Examining the variance and distribution between each feature\")\n",
    "    for i in range(1, pd_data.shape[1]):\n",
    "        for j in range(1, pd_data.shape[1]):\n",
    "            \n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                x_axis = pd_data.columns.values[i]\n",
    "                y_axis = pd_data.columns.values[j]\n",
    "                \n",
    "                top = \"Plotting X-axis: \" +  x_axis + \" Vs. Y-axis: \" + y_axis\n",
    "                plt.title(top)\n",
    "                plt.xlabel(x_axis)\n",
    "                plt.ylabel(y_axis)\n",
    "                plt.plot(pd_data[x_axis][:100], pd_data[y_axis][:100], 'r-')\n",
    "                plt.show()\n",
    "    print(\"################################################\\n\\n\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features of the each dataset are as follows:\n",
      "['Type' 'Time (ms)' ' X' ' Y' ' Z']\n",
      "################################################\n",
      "\n",
      "\n",
      "Shape of each dataset:\n",
      "(20704, 5)\n",
      "################################################\n",
      "\n",
      "\n",
      "The first 10 rows of the file are: \n",
      "            Type  Time (ms)         X         Y         Z\n",
      "0  ACCELEROMETER          0 -6.237086  7.429435 -1.201926\n",
      "1      GYROSCOPE          1  0.090408 -0.108734 -0.024435\n",
      "2      GYROSCOPE          1  0.086743 -0.103847 -0.024435\n",
      "3        GRAVITY          2 -6.125897  7.530861 -1.389070\n",
      "4  ACCELEROMETER          2 -6.225114  7.436617 -1.206715\n",
      "5  ACCELEROMETER          3 -6.210749  7.422252 -1.201926\n",
      "6      GYROSCOPE          3  0.095295 -0.108734 -0.018326\n",
      "7  ACCELEROMETER          4 -6.208354  7.434223 -1.194743\n",
      "8      GYROSCOPE          7  0.097738 -0.118508 -0.019548\n",
      "9  ACCELEROMETER         13 -6.208354  7.419858 -1.199532\n",
      "################################################\n",
      "\n",
      "\n",
      "Correlation between features of the file: \n",
      "           Time (ms)         X         Y         Z\n",
      "Time (ms)   1.000000  0.220628 -0.010272  0.390422\n",
      " X          0.220628  1.000000 -0.836707  0.494162\n",
      " Y         -0.010272 -0.836707  1.000000 -0.199578\n",
      " Z          0.390422  0.494162 -0.199578  1.000000\n",
      "################################################\n",
      "\n",
      "\n",
      "Basic statistical analysis of the dataset: \n",
      "          Time (ms)             X             Y             Z\n",
      "count  20704.000000  20704.000000  20704.000000  20704.000000\n",
      "mean    9323.774633     -2.794811      4.207110     -0.261264\n",
      "std     5435.381024      2.935813      3.823415      2.522872\n",
      "min        0.000000    -11.016058     -1.406212     -8.423059\n",
      "25%     4611.000000     -5.593026      0.155160     -1.645946\n",
      "50%     9320.500000     -1.538159      5.861185     -0.317650\n",
      "75%    14030.000000     -0.046426      7.520417      0.058643\n",
      "max    18736.000000      4.178010     13.134992     14.301003\n",
      "################################################\n",
      "\n",
      "\n",
      "Examine the heatmap of the correlations of the dataset: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examine_data(csv_dict['data-ear-1.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features of the each dataset are as follows:\n",
      "['Type' 'Time (ms)' ' X' ' Y' ' Z']\n",
      "################################################\n",
      "\n",
      "\n",
      "Shape of each dataset:\n",
      "(16330, 5)\n",
      "################################################\n",
      "\n",
      "\n",
      "The first 10 rows of the file are: \n",
      "            Type  Time (ms)         X         Y         Z\n",
      "0  ACCELEROMETER          0  6.953537  6.939178  1.665405\n",
      "1      GYROSCOPE          1 -0.322647 -0.435349 -0.469025\n",
      "2        GRAVITY          1  6.802034  6.931047  1.365030\n",
      "3  ACCELEROMETER          2  6.848251  7.051651  1.627121\n",
      "4      GYROSCOPE          2 -0.317154 -0.432907 -0.460480\n",
      "5  ACCELEROMETER          2  6.745361  7.133011  1.641479\n",
      "6      GYROSCOPE          7 -0.315323 -0.425583 -0.448853\n",
      "7  ACCELEROMETER          7  6.649658  7.168900  1.643875\n",
      "8        GRAVITY          7  6.789684  6.943693  1.362241\n",
      "9      GYROSCOPE          7 -0.315323 -0.412750 -0.432968\n",
      "################################################\n",
      "\n",
      "\n",
      "Correlation between features of the file: \n",
      "           Time (ms)         X         Y         Z\n",
      "Time (ms)   1.000000 -0.139610 -0.044340  0.095932\n",
      " X         -0.139610  1.000000  0.863780 -0.164368\n",
      " Y         -0.044340  0.863780  1.000000  0.099033\n",
      " Z          0.095932 -0.164368  0.099033  1.000000\n",
      "################################################\n",
      "\n",
      "\n",
      "Basic statistical analysis of the dataset: \n",
      "          Time (ms)             X             Y             Z\n",
      "count  16330.000000  16330.000000  16330.000000  16330.000000\n",
      "mean    7821.237600      3.062260      4.584786      0.323393\n",
      "std     4568.068907      2.993982      3.986204      1.960134\n",
      "min        0.000000     -3.936188     -6.331467     -4.517868\n",
      "25%     3873.000000      0.038361      0.059448     -0.396317\n",
      "50%     7822.000000      3.905090      6.673584     -0.014359\n",
      "75%    11776.750000      5.696008      8.066208      0.375671\n",
      "max    15732.000000      9.779465     11.676971     16.639694\n",
      "################################################\n",
      "\n",
      "\n",
      "Examine the heatmap of the correlations of the dataset: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examine_data(csv_dict['data-ear-2.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The above problems will be further examined below...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Problems involved - \n",
    "1. is the 'Type' feature important? \n",
    "2. is the 'Time (ms)' feature relevant?\n",
    "3. should the files with names - 1, 2, 3 be combined?\n",
    "'''\n",
    "print(\"The above problems will be further examined below...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with the 'Type' feature...\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ds1 = csv_dict['data-ear-1.csv']\n",
    "ds2 = csv_dict['data-ear-2.csv']\n",
    "np.random.seed(123)\n",
    "\n",
    "# testing without using 'Type'\n",
    "# number of clusters is set to 3 because of the 3 types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a. Clustering without 'Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. need for clustering?\n",
    "** helps to identify outliers\n",
    "** helps to identify class variability\n",
    "** applying k-means...\n",
    "** need other clustering algorithms?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to apply standard scalers over the dataset\n",
    "def std_scale(ds):\n",
    "    std = StandardScaler()\n",
    "    std_val = std.fit_transform(ds.iloc[:, 1:])\n",
    "    return std_val\n",
    "\n",
    "# function to apply scaling between range\n",
    "def std_norm(matrix):\n",
    "    \n",
    "    sh = matrix.shape\n",
    "    # initialise the new normalised matrix\n",
    "    norm_mat = np.zeros(sh[0] * sh[1]).reshape(sh[0], -1)\n",
    "    \n",
    "    # take out a column\n",
    "    for feature in range(sh[1]):\n",
    "        column = matrix[:, feature]\n",
    "        max_val = max(column)\n",
    "        min_val = min(column)\n",
    "        \n",
    "        # take out each row (cell in column)\n",
    "        for sample in range(sh[0]):\n",
    "            normed = (column[sample] - min_val) / (max_val - min_val)\n",
    "            norm_mat[sample, feature] = normed\n",
    "    \n",
    "    return norm_mat\n",
    "\n",
    "# function to apply kmeans over the dataset\n",
    "def k_means(ds, cls):\n",
    "    model = KMeans(n_clusters=cls)\n",
    "    model.fit(ds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunny\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\sunny\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\sunny\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\sunny\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# applying the standard scaler function\n",
    "std_ds1 = std_scale(ds1)\n",
    "std_ds2 = std_scale(ds2)\n",
    "\n",
    "# applying the normalisation function\n",
    "norm_ds1 = std_norm(np.array(ds1.iloc[:, 1:]))\n",
    "norm_ds2 = std_norm(np.array(ds2.iloc[:, 1:]))\n",
    "\n",
    "# applying the kmeans on standard scaled data\n",
    "km_ds1 = k_means(std_ds1, 3)\n",
    "km_ds2 = k_means(std_ds2, 3)\n",
    "\n",
    "# applying the kmeans on normalised data\n",
    "km0_ds1 = k_means(norm_ds1, 3)\n",
    "km0_ds2 = k_means(norm_ds2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimums:  -3.235198213281745 0.0\n",
      "Maximums:  5.77223835608355 1.0\n"
     ]
    }
   ],
   "source": [
    "# difference between standard scaler and standard norm functions...\n",
    "t = 3\n",
    "print(\"Minimums: \", min(std_ds1[:, t]), min(norm_ds1[:, t]))\n",
    "print(\"Maximums: \", max(std_ds1[:, t]), max(norm_ds1[:, t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with 3 centers using the standard scaler values...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Clustering with 3 centers using the standard scaler values...\")\n",
    "# plotting each point in the data-ear-1 versus their label\n",
    "plt.plot(km_ds1.labels_, np.arange(ds1.shape[0]), 'r.')\n",
    "plt.show()\n",
    "\n",
    "# plotting each point in the data-ear-2 versus their label\n",
    "plt.plot(km_ds2.labels_, np.arange(ds2.shape[0]), 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with 3 centers using the standard norm values...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Clustering with 3 centers using the standard norm values...\")\n",
    "\n",
    "# plotting each point in the data-ear-1 versus their label\n",
    "plt.plot(km0_ds1.labels_, np.arange(ds1.shape[0]), 'r.')\n",
    "plt.show()\n",
    "\n",
    "# plotting each point in the data-ear-2 versus their label\n",
    "plt.plot(km0_ds2.labels_, np.arange(ds2.shape[0]), 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b Clustering with 'Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform each dataset to include the 'Type' feature\n",
    "\n",
    "# function to one-hot-encode they 'Type' as it is a categorical feature\n",
    "def one_hot(ds, feature, loc):\n",
    "    # ds: the pandas dataframe to be used as input\n",
    "    \n",
    "    onehot = []\n",
    "    labset = set(ds.iloc[:, loc])\n",
    "    print(labset)\n",
    "    for s in labset:\n",
    "        lab = (ds[feature] == s).astype('int')\n",
    "        onehot.append(lab)\n",
    "    \n",
    "    return np.array(onehot).T\n",
    "\n",
    "def one_hot_join(scaled, one_hot_res):\n",
    "    # scaled: the transformed (scaled or normalized) data\n",
    "    # one_hot_res: the result returned from the one_hot function\n",
    "    \n",
    "    joint = np.concatenate((scaled, one_hot_res), axis=1)\n",
    "    return joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of datasets\n",
    "onehot_ds1 = one_hot(ds1, 'Type', 0)\n",
    "onehot_ds2 = one_hot(ds2, 'Type', 0)\n",
    "\n",
    "# join the one hot encoded values - standard scaler\n",
    "encode_ds1 = one_hot_join(std_ds1, onehot_ds1)\n",
    "encode_ds2 = one_hot_join(std_ds2, onehot_ds2)\n",
    "\n",
    "# join the one hot encoded values - normalised\n",
    "encode0_ds1 = one_hot_join(norm_ds1, onehot_ds1)\n",
    "encode0_ds2 = one_hot_join(norm_ds2, onehot_ds2)\n",
    "\n",
    "# encoded standard scaler kmeans\n",
    "km_en_ds1 = k_means(encode_ds1, 3)\n",
    "km_en_ds2 = k_means(encode_ds2, 3)\n",
    "\n",
    "# encoded normalised scaled kmeans\n",
    "km_en0_ds1 = k_means(encode0_ds1, 3)\n",
    "km_en0_ds2 = k_means(encode0_ds2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering  AFTER ENCODING with 3 centers using the standard norm values...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Clustering  AFTER ENCODING with 3 centers using the standard norm values...\")\n",
    "# plotting each point in the data-ear-1 versus their label\n",
    "plt.plot(km_en_ds1.labels_, np.arange(ds1.shape[0]), 'r.')\n",
    "plt.show()\n",
    "\n",
    "# plotting each point in the data-ear-2 versus their label\n",
    "plt.plot(km_en_ds2.labels_, np.arange(ds2.shape[0]), 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering  AFTER ENCODING with 3 centers using the standard norm values...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Clustering  AFTER ENCODING with 3 centers using the standard norm values...\")\n",
    "# plotting each point in the data-ear-1 versus their label\n",
    "plt.plot(km_en0_ds1.labels_, np.arange(ds1.shape[0]), 'r.')\n",
    "plt.show()\n",
    "\n",
    "# plotting each point in the data-ear-2 versus their label\n",
    "plt.plot(km_en0_ds2.labels_, np.arange(ds2.shape[0]), 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_en0_ds1.labels_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode0_ds1[:10, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following are the conclusions from the clustering experiments:\n",
    "1. The 'Type' feature is one-hot encoded and above results show that it is of more significance to the data\n",
    "2. The 'Time (ms)' feature is significant in relation to the x, y and z axis values (will be seen when datasets are combined)\n",
    "3. The normalization method will be chosen over the standard scaler method since it results in values between 0 and 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combining and Seperating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. testing classifiers\n",
    "** set datasets and classes\n",
    "** train the classifiers\n",
    "\n",
    "\n",
    "Goal: to create a classifier in a 5-class problem\n",
    "1. Combine all the datasets and apply one hot encoding (for the 5 classes)\n",
    "2. Keep the combined test datasets seperately\n",
    "3. Seperate the combined input datasets into train and validation sets\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a. Combine required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate train and test\n",
    "ftrain = []\n",
    "ftest = []\n",
    "\n",
    "# using 'test' word to seperate out the test data\n",
    "for f in csv_dict.keys():\n",
    "    if 'test' in f:\n",
    "        ftest.append(f)\n",
    "    else:\n",
    "        ftrain.append(f)\n",
    "\n",
    "\n",
    "# function to combine datasets\n",
    "def combine_data(fdict, flist):\n",
    "    # fdict: the dictionary that contains all the files\n",
    "    # flist: the list of files to be combined\n",
    "\n",
    "    npcat = np.array(fdict[flist[0]])\n",
    "    for i in range(1, len(flist)):\n",
    "        fi = np.array(fdict[flist[i]])\n",
    "        npcat = np.concatenate((npcat, fi), axis=0)\n",
    "    \n",
    "    return npcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = combine_data(csv_dict, ftrain)\n",
    "pd_test = combine_data(csv_dict, ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data-ear-1.csv', 'data-ear-2.csv', 'data-ear-3.csv', 'data-hand-landscape-1.csv', 'data-hand-landscape-2.csv', 'data-hand-landscape-3.csv', 'data-hand-portrait-1.csv', 'data-hand-portrait-2.csv', 'data-hand-portrait-3.csv', 'data-hand-swinging-1.csv', 'data-hand-swinging-2.csv', 'data-hand-swinging-3.csv', 'data-pocket-1.csv', 'data-pocket-2.csv', 'data-pocket-3.csv', 'test-dataset-1.csv', 'test-dataset-2.csv'])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b. One-hot encoding of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique class names:  {'data-ear-', 'data-pocket-', 'data-hand-landscape-', 'data-hand-swinging-', 'data-hand-portrait-'}\n"
     ]
    }
   ],
   "source": [
    "# now attach a class id for each dataset\n",
    "class_dict = dict()\n",
    "unique_id = []\n",
    "for f in csv_dict.keys():\n",
    "    \n",
    "    if 'test' in f:\n",
    "        # skipping the test files\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        # get the class id\n",
    "        class_id = f[:-5]\n",
    "        unique_id.append(class_id)\n",
    "\n",
    "        # create a dataframe of the id having length the same as that of the particular dataset\n",
    "        list_id = pd.DataFrame([class_id] * csv_dict[f].shape[0], columns=['class_id'])\n",
    "\n",
    "        # join the id dataframe with the dataset dataframe\n",
    "        class_dict[f] = pd.concat([csv_dict[f], list_id], axis=1)\n",
    "    \n",
    "print(\"Unique class names: \", set(unique_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Type', 'Time (ms)', ' X', ' Y', ' Z', 'class_id'], dtype=object)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class id has been added\n",
    "cols = class_dict['data-pocket-1.csv'].columns.values\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3c. Joining the pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required training data is combined:  (283034, 6)\n",
      "{'data-ear-', 'data-pocket-', 'data-hand-landscape-', 'data-hand-swinging-', 'data-hand-portrait-'}\n",
      "Encoding with 'class_id' is complete:  (283034, 5)\n",
      "{'GRAVITY', 'MAGNETOMETER', 'ACCELEROMETER', 'BAROMETER', 'GYROSCOPE'}\n",
      "Encoding with 'Type' is complete:  (283034, 5)\n",
      "Normalization of training data is complete:  (283034, 4)\n",
      "Final training set is collected!  (283034, 9)\n"
     ]
    }
   ],
   "source": [
    "# combine the labeled data\n",
    "train_class = combine_data(class_dict, ftrain)\n",
    "print(\"Required training data is combined: \", train_class.shape)\n",
    "\n",
    "# encoding each dataset with its class\n",
    "encode_class = one_hot(pd.DataFrame(train_class, columns=cols), 'class_id', 5)\n",
    "print(\"Encoding with 'class_id' is complete: \", encode_class.shape)\n",
    "\n",
    "# encoding each dataset with its 'Type'\n",
    "encode_type = one_hot(pd.DataFrame(train_class, columns=cols), 'Type', 0)\n",
    "print(\"Encoding with 'Type' is complete: \", encode_type.shape)\n",
    "                \n",
    "# normalize the dataset\n",
    "norm_train = std_norm(pd_train[:, 1:])\n",
    "print(\"Normalization of training data is complete: \", norm_train.shape)\n",
    "\n",
    "# join the encoding\n",
    "train_data = np.concatenate((encode_type, norm_train), axis=1)\n",
    "print(\"Final training set is collected! \", train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building and Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. classifier result analysis\n",
    "** examine predictions\n",
    "** study residuals \n",
    "** try different testing metrics\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes:  (254730, 9) (254730, 5)\n",
      "Test shapes:  (28304, 9) (28304, 5)\n",
      "Train on 178311 samples, validate on 76419 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.1183 - acc: 0.5172 - val_loss: 0.1002 - val_acc: 0.5950\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0939 - acc: 0.6001 - val_loss: 0.0897 - val_acc: 0.6022\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0877 - acc: 0.6277 - val_loss: 0.0860 - val_acc: 0.6480\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0845 - acc: 0.6457 - val_loss: 0.0835 - val_acc: 0.6614\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0820 - acc: 0.6614 - val_loss: 0.0813 - val_acc: 0.6637\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0801 - acc: 0.6761 - val_loss: 0.0786 - val_acc: 0.6888\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0784 - acc: 0.6880 - val_loss: 0.0784 - val_acc: 0.6845\n",
      "Epoch 8/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0770 - acc: 0.6947 - val_loss: 0.0776 - val_acc: 0.6880\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0756 - acc: 0.6978 - val_loss: 0.0746 - val_acc: 0.6991\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0743 - acc: 0.7024 - val_loss: 0.0736 - val_acc: 0.7055\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 13s 76us/step - loss: 0.0733 - acc: 0.7059 - val_loss: 0.0733 - val_acc: 0.7019\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0725 - acc: 0.7086 - val_loss: 0.0736 - val_acc: 0.7044\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0719 - acc: 0.7099 - val_loss: 0.0721 - val_acc: 0.7052\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0714 - acc: 0.7130 - val_loss: 0.0712 - val_acc: 0.7148\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0709 - acc: 0.7142 - val_loss: 0.0704 - val_acc: 0.7162\n",
      "Epoch 16/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0704 - acc: 0.7151 - val_loss: 0.0708 - val_acc: 0.7182\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0699 - acc: 0.7174 - val_loss: 0.0711 - val_acc: 0.7177\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0694 - acc: 0.7191 - val_loss: 0.0696 - val_acc: 0.7244\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0690 - acc: 0.7207 - val_loss: 0.0693 - val_acc: 0.7233\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0686 - acc: 0.7225 - val_loss: 0.0680 - val_acc: 0.7280\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0683 - acc: 0.7239 - val_loss: 0.0690 - val_acc: 0.7197\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0679 - acc: 0.7250 - val_loss: 0.0677 - val_acc: 0.7246\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0676 - acc: 0.7261 - val_loss: 0.0671 - val_acc: 0.7300\n",
      "Epoch 24/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0672 - acc: 0.7282 - val_loss: 0.0676 - val_acc: 0.7196\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0670 - acc: 0.7286 - val_loss: 0.0667 - val_acc: 0.7291\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0667 - acc: 0.7286 - val_loss: 0.0667 - val_acc: 0.7255\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0665 - acc: 0.7294 - val_loss: 0.0671 - val_acc: 0.7299\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 13s 76us/step - loss: 0.0663 - acc: 0.7304 - val_loss: 0.0657 - val_acc: 0.7371\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0661 - acc: 0.7313 - val_loss: 0.0657 - val_acc: 0.7330\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0660 - acc: 0.7330 - val_loss: 0.0667 - val_acc: 0.7276\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0658 - acc: 0.7325 - val_loss: 0.0667 - val_acc: 0.7263\n",
      "Epoch 32/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0656 - acc: 0.7341 - val_loss: 0.0677 - val_acc: 0.7190\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0655 - acc: 0.7351 - val_loss: 0.0655 - val_acc: 0.7318\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0654 - acc: 0.7361 - val_loss: 0.0653 - val_acc: 0.7379\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0653 - acc: 0.7362 - val_loss: 0.0648 - val_acc: 0.7396\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0652 - acc: 0.7371 - val_loss: 0.0655 - val_acc: 0.7434\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 13s 76us/step - loss: 0.0651 - acc: 0.7379 - val_loss: 0.0651 - val_acc: 0.7383\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0650 - acc: 0.7369 - val_loss: 0.0647 - val_acc: 0.7433\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0649 - acc: 0.7384 - val_loss: 0.0647 - val_acc: 0.7446\n",
      "Epoch 40/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0648 - acc: 0.7389 - val_loss: 0.0652 - val_acc: 0.7363\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 13s 76us/step - loss: 0.0647 - acc: 0.7395 - val_loss: 0.0659 - val_acc: 0.7293\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0645 - acc: 0.7393 - val_loss: 0.0669 - val_acc: 0.7287\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0645 - acc: 0.7396 - val_loss: 0.0639 - val_acc: 0.7376\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0644 - acc: 0.7410 - val_loss: 0.0642 - val_acc: 0.7417\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0644 - acc: 0.7401 - val_loss: 0.0637 - val_acc: 0.7485\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0643 - acc: 0.7403 - val_loss: 0.0640 - val_acc: 0.7449\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0642 - acc: 0.7406 - val_loss: 0.0649 - val_acc: 0.7375\n",
      "Epoch 48/100\n",
      "178311/178311 [==============================] - 15s 87us/step - loss: 0.0641 - acc: 0.7405 - val_loss: 0.0633 - val_acc: 0.7471\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0641 - acc: 0.7419 - val_loss: 0.0643 - val_acc: 0.7372\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0640 - acc: 0.7405 - val_loss: 0.0651 - val_acc: 0.7376\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0639 - acc: 0.7407 - val_loss: 0.0643 - val_acc: 0.7455\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0638 - acc: 0.7425 - val_loss: 0.0645 - val_acc: 0.7373\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0637 - acc: 0.7427 - val_loss: 0.0642 - val_acc: 0.7433\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0636 - acc: 0.7423 - val_loss: 0.0638 - val_acc: 0.7460\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0636 - acc: 0.7435 - val_loss: 0.0642 - val_acc: 0.7396\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0635 - acc: 0.7428 - val_loss: 0.0636 - val_acc: 0.7428\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0635 - acc: 0.7433 - val_loss: 0.0629 - val_acc: 0.7425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0634 - acc: 0.7428 - val_loss: 0.0637 - val_acc: 0.7431\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0633 - acc: 0.7442 - val_loss: 0.0633 - val_acc: 0.7428\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0633 - acc: 0.7431 - val_loss: 0.0628 - val_acc: 0.7481\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0632 - acc: 0.7445 - val_loss: 0.0642 - val_acc: 0.7382\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0631 - acc: 0.7440 - val_loss: 0.0636 - val_acc: 0.7427\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0631 - acc: 0.7442 - val_loss: 0.0642 - val_acc: 0.7319\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0631 - acc: 0.7444 - val_loss: 0.0638 - val_acc: 0.7417\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0629 - acc: 0.7441 - val_loss: 0.0633 - val_acc: 0.7398\n",
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0629 - acc: 0.7454 - val_loss: 0.0628 - val_acc: 0.7449\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0629 - acc: 0.7441 - val_loss: 0.0627 - val_acc: 0.7506\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0629 - acc: 0.7447 - val_loss: 0.0625 - val_acc: 0.7529\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0627 - acc: 0.7461 - val_loss: 0.0626 - val_acc: 0.7474\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0627 - acc: 0.7460 - val_loss: 0.0629 - val_acc: 0.7423\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0627 - acc: 0.7457 - val_loss: 0.0643 - val_acc: 0.7346\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0626 - acc: 0.7464 - val_loss: 0.0630 - val_acc: 0.7513\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0625 - acc: 0.7473 - val_loss: 0.0628 - val_acc: 0.7472\n",
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0625 - acc: 0.7475 - val_loss: 0.0628 - val_acc: 0.7472\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0625 - acc: 0.7469 - val_loss: 0.0623 - val_acc: 0.7503\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0625 - acc: 0.7466 - val_loss: 0.0640 - val_acc: 0.7332\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0624 - acc: 0.7465 - val_loss: 0.0621 - val_acc: 0.7520\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0624 - acc: 0.7478 - val_loss: 0.0618 - val_acc: 0.7498\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0624 - acc: 0.7472 - val_loss: 0.0622 - val_acc: 0.7529\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0623 - acc: 0.7474 - val_loss: 0.0619 - val_acc: 0.7534\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0623 - acc: 0.7476 - val_loss: 0.0640 - val_acc: 0.7268\n",
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0622 - acc: 0.7480 - val_loss: 0.0622 - val_acc: 0.7511\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0621 - acc: 0.7487 - val_loss: 0.0630 - val_acc: 0.7468\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0621 - acc: 0.7482 - val_loss: 0.0615 - val_acc: 0.7548\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0621 - acc: 0.7481 - val_loss: 0.0616 - val_acc: 0.7531\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0621 - acc: 0.7487 - val_loss: 0.0612 - val_acc: 0.7507\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0621 - acc: 0.7478 - val_loss: 0.0636 - val_acc: 0.7382\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0620 - acc: 0.7480 - val_loss: 0.0616 - val_acc: 0.7472\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0619 - acc: 0.7484 - val_loss: 0.0624 - val_acc: 0.7465\n",
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 16s 87us/step - loss: 0.0619 - acc: 0.7492 - val_loss: 0.0622 - val_acc: 0.7500\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 16s 87us/step - loss: 0.0619 - acc: 0.7489 - val_loss: 0.0617 - val_acc: 0.7520\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 16s 88us/step - loss: 0.0619 - acc: 0.7484 - val_loss: 0.0611 - val_acc: 0.7550\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0619 - acc: 0.7486 - val_loss: 0.0623 - val_acc: 0.7408\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0619 - acc: 0.7485 - val_loss: 0.0613 - val_acc: 0.7557\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0618 - acc: 0.7484 - val_loss: 0.0616 - val_acc: 0.7480\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0618 - acc: 0.7497 - val_loss: 0.0627 - val_acc: 0.7493\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0618 - acc: 0.7484 - val_loss: 0.0612 - val_acc: 0.7519\n",
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0617 - acc: 0.7490 - val_loss: 0.0623 - val_acc: 0.7409\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0617 - acc: 0.7492 - val_loss: 0.0610 - val_acc: 0.7566\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0617 - acc: 0.7491 - val_loss: 0.0611 - val_acc: 0.7526\n",
      "28304/28304 [==============================] - 1s 47us/step\n",
      "\n",
      "Loss: 0.06086042340989122\n",
      "\n",
      "Accuracy: 75.23318258903335%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  1  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n",
      "Train shapes:  (254730, 9) (254730, 5)\n",
      "Test shapes:  (28304, 9) (28304, 5)\n",
      "Train on 178311 samples, validate on 76419 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 17s 98us/step - loss: 0.1167 - acc: 0.5073 - val_loss: 0.0990 - val_acc: 0.5829\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0934 - acc: 0.5965 - val_loss: 0.0904 - val_acc: 0.5912\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 17s 96us/step - loss: 0.0878 - acc: 0.6105 - val_loss: 0.0875 - val_acc: 0.6108\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 17s 96us/step - loss: 0.0844 - acc: 0.6347 - val_loss: 0.0834 - val_acc: 0.6472\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0806 - acc: 0.6666 - val_loss: 0.0787 - val_acc: 0.6809\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0771 - acc: 0.6840 - val_loss: 0.0777 - val_acc: 0.6848\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0749 - acc: 0.6950 - val_loss: 0.0743 - val_acc: 0.7088\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0736 - acc: 0.7021 - val_loss: 0.0751 - val_acc: 0.6960\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0726 - acc: 0.7065 - val_loss: 0.0719 - val_acc: 0.7134\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0718 - acc: 0.7103 - val_loss: 0.0721 - val_acc: 0.7022\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0711 - acc: 0.7118 - val_loss: 0.0714 - val_acc: 0.7110\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0705 - acc: 0.7136 - val_loss: 0.0715 - val_acc: 0.7101\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0701 - acc: 0.7147 - val_loss: 0.0695 - val_acc: 0.7234\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0697 - acc: 0.7158 - val_loss: 0.0715 - val_acc: 0.7061\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0693 - acc: 0.7178 - val_loss: 0.0687 - val_acc: 0.7212\n",
      "Epoch 16/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0689 - acc: 0.7192 - val_loss: 0.0717 - val_acc: 0.7061\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0686 - acc: 0.7205 - val_loss: 0.0705 - val_acc: 0.7145\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0682 - acc: 0.7221 - val_loss: 0.0699 - val_acc: 0.7157\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0679 - acc: 0.7230 - val_loss: 0.0684 - val_acc: 0.7208\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0676 - acc: 0.7245 - val_loss: 0.0687 - val_acc: 0.7137\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0672 - acc: 0.7254 - val_loss: 0.0675 - val_acc: 0.7244\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0668 - acc: 0.7267 - val_loss: 0.0665 - val_acc: 0.7320\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0666 - acc: 0.7271 - val_loss: 0.0691 - val_acc: 0.6909\n",
      "Epoch 24/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0663 - acc: 0.7284 - val_loss: 0.0660 - val_acc: 0.7275\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0660 - acc: 0.7301 - val_loss: 0.0665 - val_acc: 0.7283\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0659 - acc: 0.7304 - val_loss: 0.0685 - val_acc: 0.7064\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0656 - acc: 0.7311 - val_loss: 0.0674 - val_acc: 0.7313\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0655 - acc: 0.7324 - val_loss: 0.0673 - val_acc: 0.7226\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0651 - acc: 0.7335 - val_loss: 0.0658 - val_acc: 0.7256\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0650 - acc: 0.7343 - val_loss: 0.0647 - val_acc: 0.7334\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0647 - acc: 0.7359 - val_loss: 0.0657 - val_acc: 0.7244\n",
      "Epoch 32/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0645 - acc: 0.7365 - val_loss: 0.0649 - val_acc: 0.7334\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 16s 87us/step - loss: 0.0644 - acc: 0.7370 - val_loss: 0.0645 - val_acc: 0.7400\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0642 - acc: 0.7375 - val_loss: 0.0650 - val_acc: 0.7367\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 16s 93us/step - loss: 0.0641 - acc: 0.7378 - val_loss: 0.0644 - val_acc: 0.7348\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0640 - acc: 0.7386 - val_loss: 0.0644 - val_acc: 0.7402\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0638 - acc: 0.7390 - val_loss: 0.0634 - val_acc: 0.7402\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0638 - acc: 0.7389 - val_loss: 0.0645 - val_acc: 0.7411\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0637 - acc: 0.7393 - val_loss: 0.0636 - val_acc: 0.7437\n",
      "Epoch 40/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0635 - acc: 0.7408 - val_loss: 0.0641 - val_acc: 0.7436\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0636 - acc: 0.7399 - val_loss: 0.0635 - val_acc: 0.7436\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0635 - acc: 0.7407 - val_loss: 0.0673 - val_acc: 0.7132\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0634 - acc: 0.7407 - val_loss: 0.0640 - val_acc: 0.7406\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0633 - acc: 0.7411 - val_loss: 0.0632 - val_acc: 0.7479\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0632 - acc: 0.7422 - val_loss: 0.0639 - val_acc: 0.7369\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0631 - acc: 0.7424 - val_loss: 0.0633 - val_acc: 0.7414\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0631 - acc: 0.7417 - val_loss: 0.0641 - val_acc: 0.7372\n",
      "Epoch 48/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0630 - acc: 0.7431 - val_loss: 0.0639 - val_acc: 0.7351\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0629 - acc: 0.7437 - val_loss: 0.0633 - val_acc: 0.7458\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0627 - acc: 0.7428 - val_loss: 0.0637 - val_acc: 0.7371\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0627 - acc: 0.7443 - val_loss: 0.0636 - val_acc: 0.7353\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0626 - acc: 0.7456 - val_loss: 0.0655 - val_acc: 0.7166\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0625 - acc: 0.7455 - val_loss: 0.0629 - val_acc: 0.7475\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0624 - acc: 0.7458 - val_loss: 0.0634 - val_acc: 0.7426\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0622 - acc: 0.7457 - val_loss: 0.0631 - val_acc: 0.7327\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0623 - acc: 0.7456 - val_loss: 0.0629 - val_acc: 0.7462\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0623 - acc: 0.7450 - val_loss: 0.0620 - val_acc: 0.7462\n",
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 17s 96us/step - loss: 0.0621 - acc: 0.7456 - val_loss: 0.0614 - val_acc: 0.7474\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0620 - acc: 0.7468 - val_loss: 0.0637 - val_acc: 0.7393\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0621 - acc: 0.7469 - val_loss: 0.0627 - val_acc: 0.7500\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0620 - acc: 0.7464 - val_loss: 0.0628 - val_acc: 0.7416\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0620 - acc: 0.7477 - val_loss: 0.0629 - val_acc: 0.7467\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0619 - acc: 0.7482 - val_loss: 0.0619 - val_acc: 0.7449\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 18s 99us/step - loss: 0.0619 - acc: 0.7485 - val_loss: 0.0617 - val_acc: 0.7462\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0618 - acc: 0.7479 - val_loss: 0.0634 - val_acc: 0.7294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0617 - acc: 0.7475 - val_loss: 0.0616 - val_acc: 0.7409\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0617 - acc: 0.7484 - val_loss: 0.0632 - val_acc: 0.7404\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0617 - acc: 0.7495 - val_loss: 0.0624 - val_acc: 0.7459\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0616 - acc: 0.7491 - val_loss: 0.0622 - val_acc: 0.7521\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0615 - acc: 0.7494 - val_loss: 0.0614 - val_acc: 0.7522\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0614 - acc: 0.7492 - val_loss: 0.0617 - val_acc: 0.7503\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0614 - acc: 0.7491 - val_loss: 0.0621 - val_acc: 0.7464\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0614 - acc: 0.7487 - val_loss: 0.0619 - val_acc: 0.7468\n",
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0614 - acc: 0.7496 - val_loss: 0.0610 - val_acc: 0.7524\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0614 - acc: 0.7498 - val_loss: 0.0611 - val_acc: 0.7511\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0612 - acc: 0.7509 - val_loss: 0.0613 - val_acc: 0.7513\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0612 - acc: 0.7510 - val_loss: 0.0617 - val_acc: 0.7494\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0612 - acc: 0.7512 - val_loss: 0.0618 - val_acc: 0.7466\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0611 - acc: 0.7516 - val_loss: 0.0619 - val_acc: 0.7460\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0611 - acc: 0.7514 - val_loss: 0.0612 - val_acc: 0.7513\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0610 - acc: 0.7511 - val_loss: 0.0622 - val_acc: 0.7436\n",
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0609 - acc: 0.7522 - val_loss: 0.0603 - val_acc: 0.7557\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0609 - acc: 0.7516 - val_loss: 0.0610 - val_acc: 0.7526\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0609 - acc: 0.7525 - val_loss: 0.0622 - val_acc: 0.7434\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0608 - acc: 0.7523 - val_loss: 0.0604 - val_acc: 0.7556\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0607 - acc: 0.7528 - val_loss: 0.0618 - val_acc: 0.7433\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0607 - acc: 0.7532 - val_loss: 0.0603 - val_acc: 0.7549\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0607 - acc: 0.7529 - val_loss: 0.0606 - val_acc: 0.7532\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0606 - acc: 0.7538 - val_loss: 0.0630 - val_acc: 0.7402\n",
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0605 - acc: 0.7541 - val_loss: 0.0608 - val_acc: 0.7508\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0604 - acc: 0.7535 - val_loss: 0.0608 - val_acc: 0.7572\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0604 - acc: 0.7531 - val_loss: 0.0609 - val_acc: 0.7507\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0603 - acc: 0.7546 - val_loss: 0.0619 - val_acc: 0.7427\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0603 - acc: 0.7547 - val_loss: 0.0612 - val_acc: 0.7527\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0602 - acc: 0.7544 - val_loss: 0.0605 - val_acc: 0.7541\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0601 - acc: 0.7557 - val_loss: 0.0604 - val_acc: 0.7506\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0601 - acc: 0.7558 - val_loss: 0.0608 - val_acc: 0.7538\n",
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0601 - acc: 0.7555 - val_loss: 0.0622 - val_acc: 0.7364\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0600 - acc: 0.7569 - val_loss: 0.0602 - val_acc: 0.7590\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0599 - acc: 0.7571 - val_loss: 0.0605 - val_acc: 0.7528\n",
      "28304/28304 [==============================] - 1s 41us/step\n",
      "\n",
      "Loss: 0.059165786544820256\n",
      "\n",
      "Accuracy: 75.87266817410966%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  2  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n",
      "Train shapes:  (254730, 9) (254730, 5)\n",
      "Test shapes:  (28304, 9) (28304, 5)\n",
      "Train on 178311 samples, validate on 76419 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 17s 97us/step - loss: 0.1162 - acc: 0.5370 - val_loss: 0.0984 - val_acc: 0.5864\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0925 - acc: 0.6179 - val_loss: 0.0894 - val_acc: 0.6389\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0873 - acc: 0.6419 - val_loss: 0.0857 - val_acc: 0.6523\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0847 - acc: 0.6533 - val_loss: 0.0836 - val_acc: 0.6589\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0822 - acc: 0.6652 - val_loss: 0.0817 - val_acc: 0.6677\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0797 - acc: 0.6788 - val_loss: 0.0813 - val_acc: 0.6687\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0773 - acc: 0.6962 - val_loss: 0.0761 - val_acc: 0.6974\n",
      "Epoch 8/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0755 - acc: 0.7054 - val_loss: 0.0746 - val_acc: 0.7188\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0744 - acc: 0.7097 - val_loss: 0.0741 - val_acc: 0.7164\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0735 - acc: 0.7129 - val_loss: 0.0740 - val_acc: 0.7045\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0730 - acc: 0.7144 - val_loss: 0.0725 - val_acc: 0.7271\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0726 - acc: 0.7149 - val_loss: 0.0727 - val_acc: 0.7170\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0722 - acc: 0.7166 - val_loss: 0.0746 - val_acc: 0.6967\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0718 - acc: 0.7183 - val_loss: 0.0718 - val_acc: 0.7138\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0717 - acc: 0.7181 - val_loss: 0.0709 - val_acc: 0.7222\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0713 - acc: 0.7187 - val_loss: 0.0714 - val_acc: 0.7218\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0711 - acc: 0.7202 - val_loss: 0.0710 - val_acc: 0.7194\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 16s 88us/step - loss: 0.0709 - acc: 0.7193 - val_loss: 0.0709 - val_acc: 0.7244\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0707 - acc: 0.7209 - val_loss: 0.0717 - val_acc: 0.7104\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0706 - acc: 0.7208 - val_loss: 0.0716 - val_acc: 0.7169\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0704 - acc: 0.7210 - val_loss: 0.0732 - val_acc: 0.7050\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0702 - acc: 0.7214 - val_loss: 0.0710 - val_acc: 0.7172\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0701 - acc: 0.7225 - val_loss: 0.0702 - val_acc: 0.7192\n",
      "Epoch 24/100\n",
      "178311/178311 [==============================] - 17s 98us/step - loss: 0.0699 - acc: 0.7224 - val_loss: 0.0695 - val_acc: 0.7241\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 18s 99us/step - loss: 0.0698 - acc: 0.7223 - val_loss: 0.0693 - val_acc: 0.7234\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 17s 96us/step - loss: 0.0698 - acc: 0.7220 - val_loss: 0.0705 - val_acc: 0.7272\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 18s 100us/step - loss: 0.0695 - acc: 0.7244 - val_loss: 0.0703 - val_acc: 0.7223\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0694 - acc: 0.7244 - val_loss: 0.0730 - val_acc: 0.7052\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0694 - acc: 0.7250 - val_loss: 0.0715 - val_acc: 0.7124\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0693 - acc: 0.7238 - val_loss: 0.0708 - val_acc: 0.7140\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0692 - acc: 0.7244 - val_loss: 0.0687 - val_acc: 0.7266\n",
      "Epoch 32/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0692 - acc: 0.7242 - val_loss: 0.0682 - val_acc: 0.7312\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0690 - acc: 0.7254 - val_loss: 0.0693 - val_acc: 0.7179\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 16s 88us/step - loss: 0.0690 - acc: 0.7252 - val_loss: 0.0688 - val_acc: 0.7233\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 16s 88us/step - loss: 0.0688 - acc: 0.7254 - val_loss: 0.0699 - val_acc: 0.7177\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 18s 99us/step - loss: 0.0688 - acc: 0.7261 - val_loss: 0.0692 - val_acc: 0.7304\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0688 - acc: 0.7253 - val_loss: 0.0690 - val_acc: 0.7154\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0688 - acc: 0.7254 - val_loss: 0.0680 - val_acc: 0.7376\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0686 - acc: 0.7260 - val_loss: 0.0678 - val_acc: 0.7311\n",
      "Epoch 40/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0686 - acc: 0.7261 - val_loss: 0.0715 - val_acc: 0.7125\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0686 - acc: 0.7264 - val_loss: 0.0698 - val_acc: 0.7185\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 16s 87us/step - loss: 0.0685 - acc: 0.7259 - val_loss: 0.0683 - val_acc: 0.7263\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0685 - acc: 0.7269 - val_loss: 0.0695 - val_acc: 0.7276\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0684 - acc: 0.7266 - val_loss: 0.0684 - val_acc: 0.7265\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 15s 87us/step - loss: 0.0684 - acc: 0.7262 - val_loss: 0.0679 - val_acc: 0.7309\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0683 - acc: 0.7268 - val_loss: 0.0674 - val_acc: 0.7309\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0683 - acc: 0.7270 - val_loss: 0.0695 - val_acc: 0.7127\n",
      "Epoch 48/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0682 - acc: 0.7271 - val_loss: 0.0692 - val_acc: 0.7224\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0683 - acc: 0.7268 - val_loss: 0.0674 - val_acc: 0.7315\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0681 - acc: 0.7280 - val_loss: 0.0688 - val_acc: 0.7289\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0681 - acc: 0.7270 - val_loss: 0.0702 - val_acc: 0.7077\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0681 - acc: 0.7282 - val_loss: 0.0676 - val_acc: 0.7321\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0680 - acc: 0.7287 - val_loss: 0.0694 - val_acc: 0.7262\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0680 - acc: 0.7293 - val_loss: 0.0688 - val_acc: 0.7254\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0680 - acc: 0.7286 - val_loss: 0.0673 - val_acc: 0.7331\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0679 - acc: 0.7288 - val_loss: 0.0691 - val_acc: 0.7139\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0679 - acc: 0.7281 - val_loss: 0.0686 - val_acc: 0.7258\n",
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0679 - acc: 0.7286 - val_loss: 0.0681 - val_acc: 0.7254\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0678 - acc: 0.7297 - val_loss: 0.0681 - val_acc: 0.7304\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0677 - acc: 0.7306 - val_loss: 0.0691 - val_acc: 0.7269\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0677 - acc: 0.7299 - val_loss: 0.0674 - val_acc: 0.7390\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0677 - acc: 0.7291 - val_loss: 0.0680 - val_acc: 0.7262\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 16s 87us/step - loss: 0.0676 - acc: 0.7305 - val_loss: 0.0696 - val_acc: 0.7187\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0676 - acc: 0.7309 - val_loss: 0.0676 - val_acc: 0.7309\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0676 - acc: 0.7311 - val_loss: 0.0683 - val_acc: 0.7314\n",
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0675 - acc: 0.7313 - val_loss: 0.0682 - val_acc: 0.7255\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0675 - acc: 0.7303 - val_loss: 0.0674 - val_acc: 0.7286\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 14s 81us/step - loss: 0.0674 - acc: 0.7303 - val_loss: 0.0666 - val_acc: 0.7338\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0673 - acc: 0.7309 - val_loss: 0.0685 - val_acc: 0.7271\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 18s 99us/step - loss: 0.0674 - acc: 0.7313 - val_loss: 0.0670 - val_acc: 0.7345\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0673 - acc: 0.7311 - val_loss: 0.0668 - val_acc: 0.7305\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0673 - acc: 0.7317 - val_loss: 0.0677 - val_acc: 0.7257\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0673 - acc: 0.7307 - val_loss: 0.0686 - val_acc: 0.7228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0673 - acc: 0.7318 - val_loss: 0.0665 - val_acc: 0.7390\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0672 - acc: 0.7310 - val_loss: 0.0677 - val_acc: 0.7339\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0672 - acc: 0.7311 - val_loss: 0.0687 - val_acc: 0.7254\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0671 - acc: 0.7313 - val_loss: 0.0669 - val_acc: 0.7349\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0671 - acc: 0.7320 - val_loss: 0.0675 - val_acc: 0.7292\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0670 - acc: 0.7324 - val_loss: 0.0667 - val_acc: 0.7329\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0670 - acc: 0.7319 - val_loss: 0.0664 - val_acc: 0.7350\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0669 - acc: 0.7331 - val_loss: 0.0668 - val_acc: 0.7370\n",
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0669 - acc: 0.7329 - val_loss: 0.0664 - val_acc: 0.7364\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0669 - acc: 0.7333 - val_loss: 0.0665 - val_acc: 0.7377\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0669 - acc: 0.7332 - val_loss: 0.0675 - val_acc: 0.7370\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0669 - acc: 0.7330 - val_loss: 0.0684 - val_acc: 0.7273\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0668 - acc: 0.7335 - val_loss: 0.0670 - val_acc: 0.7357\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0667 - acc: 0.7330 - val_loss: 0.0667 - val_acc: 0.7337\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0668 - acc: 0.7332 - val_loss: 0.0665 - val_acc: 0.7350\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0666 - acc: 0.7343 - val_loss: 0.0671 - val_acc: 0.7203\n",
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0666 - acc: 0.7336 - val_loss: 0.0670 - val_acc: 0.7283\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0665 - acc: 0.7344 - val_loss: 0.0677 - val_acc: 0.7291\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0665 - acc: 0.7339 - val_loss: 0.0663 - val_acc: 0.7320\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0665 - acc: 0.7349 - val_loss: 0.0660 - val_acc: 0.7390\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0664 - acc: 0.7346 - val_loss: 0.0659 - val_acc: 0.7406\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0664 - acc: 0.7346 - val_loss: 0.0665 - val_acc: 0.7380\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0663 - acc: 0.7349 - val_loss: 0.0678 - val_acc: 0.7255\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0663 - acc: 0.7342 - val_loss: 0.0669 - val_acc: 0.7301\n",
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0661 - acc: 0.7356 - val_loss: 0.0672 - val_acc: 0.7292\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0662 - acc: 0.7349 - val_loss: 0.0694 - val_acc: 0.7192\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0661 - acc: 0.7355 - val_loss: 0.0660 - val_acc: 0.7353\n",
      "28304/28304 [==============================] - 1s 41us/step\n",
      "\n",
      "Loss: 0.06585255669668914\n",
      "\n",
      "Accuracy: 73.32532504239684%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  3  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n",
      "Train shapes:  (254730, 9) (254730, 5)\n",
      "Test shapes:  (28304, 9) (28304, 5)\n",
      "Train on 178311 samples, validate on 76419 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 18s 99us/step - loss: 0.1199 - acc: 0.5256 - val_loss: 0.0986 - val_acc: 0.6148\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0895 - acc: 0.6443 - val_loss: 0.0847 - val_acc: 0.6653\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0836 - acc: 0.6658 - val_loss: 0.0824 - val_acc: 0.6689\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0805 - acc: 0.6805 - val_loss: 0.0786 - val_acc: 0.6959\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0776 - acc: 0.6941 - val_loss: 0.0757 - val_acc: 0.7089\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0754 - acc: 0.7030 - val_loss: 0.0745 - val_acc: 0.7070\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 16s 93us/step - loss: 0.0743 - acc: 0.7072 - val_loss: 0.0741 - val_acc: 0.7128\n",
      "Epoch 8/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0734 - acc: 0.7097 - val_loss: 0.0735 - val_acc: 0.7105\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 17s 96us/step - loss: 0.0728 - acc: 0.7111 - val_loss: 0.0722 - val_acc: 0.7084\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 18s 98us/step - loss: 0.0723 - acc: 0.7128 - val_loss: 0.0714 - val_acc: 0.7200\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0720 - acc: 0.7134 - val_loss: 0.0711 - val_acc: 0.7220\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0717 - acc: 0.7137 - val_loss: 0.0705 - val_acc: 0.7220\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0714 - acc: 0.7155 - val_loss: 0.0709 - val_acc: 0.7207\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0712 - acc: 0.7166 - val_loss: 0.0707 - val_acc: 0.7182\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0710 - acc: 0.7173 - val_loss: 0.0708 - val_acc: 0.7053\n",
      "Epoch 16/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0707 - acc: 0.7172 - val_loss: 0.0695 - val_acc: 0.7225\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0705 - acc: 0.7183 - val_loss: 0.0704 - val_acc: 0.7220\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0703 - acc: 0.7186 - val_loss: 0.0705 - val_acc: 0.7209\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0702 - acc: 0.7187 - val_loss: 0.0696 - val_acc: 0.7240\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0700 - acc: 0.7204 - val_loss: 0.0719 - val_acc: 0.7109\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0698 - acc: 0.7201 - val_loss: 0.0690 - val_acc: 0.7244\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0697 - acc: 0.7209 - val_loss: 0.0698 - val_acc: 0.7208\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0695 - acc: 0.7207 - val_loss: 0.0706 - val_acc: 0.7167\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0693 - acc: 0.7218 - val_loss: 0.0711 - val_acc: 0.7020\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0692 - acc: 0.7220 - val_loss: 0.0680 - val_acc: 0.7263\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0690 - acc: 0.7217 - val_loss: 0.0690 - val_acc: 0.7233\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0689 - acc: 0.7230 - val_loss: 0.0684 - val_acc: 0.7259\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 16s 88us/step - loss: 0.0687 - acc: 0.7235 - val_loss: 0.0680 - val_acc: 0.7211\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 16s 87us/step - loss: 0.0685 - acc: 0.7248 - val_loss: 0.0693 - val_acc: 0.7136\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0684 - acc: 0.7239 - val_loss: 0.0699 - val_acc: 0.7167\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 17s 95us/step - loss: 0.0680 - acc: 0.7264 - val_loss: 0.0674 - val_acc: 0.7290\n",
      "Epoch 32/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0678 - acc: 0.7271 - val_loss: 0.0671 - val_acc: 0.7345\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0676 - acc: 0.7285 - val_loss: 0.0689 - val_acc: 0.7247\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0673 - acc: 0.7294 - val_loss: 0.0676 - val_acc: 0.7304\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0670 - acc: 0.7308 - val_loss: 0.0668 - val_acc: 0.7393\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0668 - acc: 0.7325 - val_loss: 0.0710 - val_acc: 0.7099\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0667 - acc: 0.7325 - val_loss: 0.0678 - val_acc: 0.7276\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0664 - acc: 0.7321 - val_loss: 0.0674 - val_acc: 0.7328\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0662 - acc: 0.7341 - val_loss: 0.0657 - val_acc: 0.7359\n",
      "Epoch 40/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0659 - acc: 0.7349 - val_loss: 0.0656 - val_acc: 0.7390\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0658 - acc: 0.7346 - val_loss: 0.0661 - val_acc: 0.7320\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0656 - acc: 0.7350 - val_loss: 0.0684 - val_acc: 0.7251\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0655 - acc: 0.7364 - val_loss: 0.0650 - val_acc: 0.7347\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0653 - acc: 0.7368 - val_loss: 0.0654 - val_acc: 0.7373\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0652 - acc: 0.7375 - val_loss: 0.0655 - val_acc: 0.7316\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0651 - acc: 0.7383 - val_loss: 0.0653 - val_acc: 0.7377\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0650 - acc: 0.7386 - val_loss: 0.0660 - val_acc: 0.7419\n",
      "Epoch 48/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0649 - acc: 0.7383 - val_loss: 0.0648 - val_acc: 0.7407\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 18s 101us/step - loss: 0.0647 - acc: 0.7403 - val_loss: 0.0642 - val_acc: 0.7360\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0646 - acc: 0.7399 - val_loss: 0.0650 - val_acc: 0.7390\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 17s 97us/step - loss: 0.0646 - acc: 0.7400 - val_loss: 0.0641 - val_acc: 0.7398\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 16s 88us/step - loss: 0.0645 - acc: 0.7402 - val_loss: 0.0635 - val_acc: 0.7463\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 16s 90us/step - loss: 0.0644 - acc: 0.7412 - val_loss: 0.0663 - val_acc: 0.7306\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 17s 98us/step - loss: 0.0643 - acc: 0.7419 - val_loss: 0.0640 - val_acc: 0.7447\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0642 - acc: 0.7426 - val_loss: 0.0635 - val_acc: 0.7525\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 17s 97us/step - loss: 0.0641 - acc: 0.7429 - val_loss: 0.0632 - val_acc: 0.7476\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 17s 98us/step - loss: 0.0641 - acc: 0.7427 - val_loss: 0.0647 - val_acc: 0.7362\n",
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0640 - acc: 0.7434 - val_loss: 0.0632 - val_acc: 0.7505\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0640 - acc: 0.7441 - val_loss: 0.0638 - val_acc: 0.7494\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0639 - acc: 0.7429 - val_loss: 0.0640 - val_acc: 0.7509\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0639 - acc: 0.7442 - val_loss: 0.0641 - val_acc: 0.7393\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0638 - acc: 0.7443 - val_loss: 0.0643 - val_acc: 0.7453\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0637 - acc: 0.7447 - val_loss: 0.0638 - val_acc: 0.7501\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0637 - acc: 0.7442 - val_loss: 0.0633 - val_acc: 0.7452\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0637 - acc: 0.7450 - val_loss: 0.0626 - val_acc: 0.7503\n",
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0636 - acc: 0.7447 - val_loss: 0.0646 - val_acc: 0.7333\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 17s 96us/step - loss: 0.0636 - acc: 0.7449 - val_loss: 0.0636 - val_acc: 0.7550\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0634 - acc: 0.7462 - val_loss: 0.0634 - val_acc: 0.7442\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0635 - acc: 0.7450 - val_loss: 0.0636 - val_acc: 0.7413\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0635 - acc: 0.7454 - val_loss: 0.0632 - val_acc: 0.7498\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 13s 76us/step - loss: 0.0635 - acc: 0.7451 - val_loss: 0.0645 - val_acc: 0.7307\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0635 - acc: 0.7444 - val_loss: 0.0643 - val_acc: 0.7467\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0634 - acc: 0.7453 - val_loss: 0.0635 - val_acc: 0.7457\n",
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0633 - acc: 0.7462 - val_loss: 0.0630 - val_acc: 0.7506\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0633 - acc: 0.7454 - val_loss: 0.0661 - val_acc: 0.7244\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0632 - acc: 0.7469 - val_loss: 0.0677 - val_acc: 0.7265\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0633 - acc: 0.7462 - val_loss: 0.0649 - val_acc: 0.7369\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0632 - acc: 0.7458 - val_loss: 0.0648 - val_acc: 0.7378\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0631 - acc: 0.7462 - val_loss: 0.0642 - val_acc: 0.7529\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0631 - acc: 0.7452 - val_loss: 0.0626 - val_acc: 0.7488\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0631 - acc: 0.7457 - val_loss: 0.0627 - val_acc: 0.7499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 9s 52us/step - loss: 0.0631 - acc: 0.7453 - val_loss: 0.0622 - val_acc: 0.7482\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 9s 50us/step - loss: 0.0631 - acc: 0.7463 - val_loss: 0.0631 - val_acc: 0.7501\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0630 - acc: 0.7470 - val_loss: 0.0624 - val_acc: 0.7537\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0630 - acc: 0.7469 - val_loss: 0.0627 - val_acc: 0.7472\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 9s 50us/step - loss: 0.0630 - acc: 0.7465 - val_loss: 0.0622 - val_acc: 0.7528\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0630 - acc: 0.7466 - val_loss: 0.0626 - val_acc: 0.7481\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0629 - acc: 0.7467 - val_loss: 0.0632 - val_acc: 0.7475\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0629 - acc: 0.7470 - val_loss: 0.0620 - val_acc: 0.7522\n",
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0628 - acc: 0.7466 - val_loss: 0.0639 - val_acc: 0.7445\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0629 - acc: 0.7465 - val_loss: 0.0621 - val_acc: 0.7556\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0628 - acc: 0.7466 - val_loss: 0.0623 - val_acc: 0.7510\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 9s 50us/step - loss: 0.0628 - acc: 0.7472 - val_loss: 0.0627 - val_acc: 0.7485\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 9s 52us/step - loss: 0.0628 - acc: 0.7471 - val_loss: 0.0630 - val_acc: 0.7472\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0628 - acc: 0.7476 - val_loss: 0.0631 - val_acc: 0.7493\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0628 - acc: 0.7472 - val_loss: 0.0635 - val_acc: 0.7435\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0628 - acc: 0.7468 - val_loss: 0.0624 - val_acc: 0.7534\n",
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 9s 50us/step - loss: 0.0627 - acc: 0.7477 - val_loss: 0.0621 - val_acc: 0.7530\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0627 - acc: 0.7483 - val_loss: 0.0628 - val_acc: 0.7481\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0627 - acc: 0.7470 - val_loss: 0.0623 - val_acc: 0.7521\n",
      "28304/28304 [==============================] - 1s 24us/step\n",
      "\n",
      "Loss: 0.06298659515707691\n",
      "\n",
      "Accuracy: 74.667891464104%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  4  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n",
      "Train shapes:  (254731, 9) (254731, 5)\n",
      "Test shapes:  (28303, 9) (28303, 5)\n",
      "Train on 178311 samples, validate on 76420 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.1167 - acc: 0.5224 - val_loss: 0.0965 - val_acc: 0.6082\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 9s 52us/step - loss: 0.0910 - acc: 0.6207 - val_loss: 0.0888 - val_acc: 0.6166\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0866 - acc: 0.6319 - val_loss: 0.0858 - val_acc: 0.6367\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 9s 52us/step - loss: 0.0850 - acc: 0.6355 - val_loss: 0.0841 - val_acc: 0.6363\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0837 - acc: 0.6402 - val_loss: 0.0834 - val_acc: 0.6440\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 9s 50us/step - loss: 0.0827 - acc: 0.6444 - val_loss: 0.0819 - val_acc: 0.6550\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0818 - acc: 0.6498 - val_loss: 0.0822 - val_acc: 0.6440\n",
      "Epoch 8/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0808 - acc: 0.6580 - val_loss: 0.0810 - val_acc: 0.6631\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 11s 59us/step - loss: 0.0798 - acc: 0.6666 - val_loss: 0.0785 - val_acc: 0.6801\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 10s 53us/step - loss: 0.0775 - acc: 0.6898 - val_loss: 0.0770 - val_acc: 0.6997\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 10s 53us/step - loss: 0.0754 - acc: 0.7041 - val_loss: 0.0756 - val_acc: 0.6920\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 9s 52us/step - loss: 0.0740 - acc: 0.7083 - val_loss: 0.0735 - val_acc: 0.7101\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0730 - acc: 0.7134 - val_loss: 0.0722 - val_acc: 0.7222\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0724 - acc: 0.7141 - val_loss: 0.0719 - val_acc: 0.7108\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0720 - acc: 0.7157 - val_loss: 0.0736 - val_acc: 0.7033\n",
      "Epoch 16/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0716 - acc: 0.7168 - val_loss: 0.0709 - val_acc: 0.7221\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0714 - acc: 0.7173 - val_loss: 0.0708 - val_acc: 0.7196\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0712 - acc: 0.7173 - val_loss: 0.0725 - val_acc: 0.7067\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0709 - acc: 0.7184 - val_loss: 0.0704 - val_acc: 0.7105\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0707 - acc: 0.7194 - val_loss: 0.0708 - val_acc: 0.7180\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0705 - acc: 0.7197 - val_loss: 0.0700 - val_acc: 0.7239\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0703 - acc: 0.7205 - val_loss: 0.0714 - val_acc: 0.7122\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0701 - acc: 0.7199 - val_loss: 0.0693 - val_acc: 0.7250\n",
      "Epoch 24/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0699 - acc: 0.7225 - val_loss: 0.0709 - val_acc: 0.7199\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0698 - acc: 0.7217 - val_loss: 0.0692 - val_acc: 0.7262\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 9s 52us/step - loss: 0.0696 - acc: 0.7224 - val_loss: 0.0707 - val_acc: 0.7221\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0694 - acc: 0.7227 - val_loss: 0.0709 - val_acc: 0.7181\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 9s 52us/step - loss: 0.0692 - acc: 0.7233 - val_loss: 0.0694 - val_acc: 0.7205\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0689 - acc: 0.7247 - val_loss: 0.0683 - val_acc: 0.7267\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0687 - acc: 0.7251 - val_loss: 0.0695 - val_acc: 0.7248\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 10s 53us/step - loss: 0.0686 - acc: 0.7255 - val_loss: 0.0676 - val_acc: 0.7309\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0682 - acc: 0.7271 - val_loss: 0.0695 - val_acc: 0.7315\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0680 - acc: 0.7282 - val_loss: 0.0677 - val_acc: 0.7240\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0679 - acc: 0.7285 - val_loss: 0.0683 - val_acc: 0.7315\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0676 - acc: 0.7293 - val_loss: 0.0677 - val_acc: 0.7299\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0673 - acc: 0.7305 - val_loss: 0.0668 - val_acc: 0.7314\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0669 - acc: 0.7329 - val_loss: 0.0685 - val_acc: 0.7221\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0666 - acc: 0.7328 - val_loss: 0.0671 - val_acc: 0.7322\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0663 - acc: 0.7338 - val_loss: 0.0654 - val_acc: 0.7409\n",
      "Epoch 40/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0660 - acc: 0.7353 - val_loss: 0.0653 - val_acc: 0.7404\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0659 - acc: 0.7357 - val_loss: 0.0667 - val_acc: 0.7348\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0657 - acc: 0.7351 - val_loss: 0.0652 - val_acc: 0.7394\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0654 - acc: 0.7364 - val_loss: 0.0653 - val_acc: 0.7374\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0654 - acc: 0.7373 - val_loss: 0.0643 - val_acc: 0.7443\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0652 - acc: 0.7380 - val_loss: 0.0647 - val_acc: 0.7371\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0651 - acc: 0.7382 - val_loss: 0.0658 - val_acc: 0.7294\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0649 - acc: 0.7385 - val_loss: 0.0646 - val_acc: 0.7420\n",
      "Epoch 48/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0647 - acc: 0.7396 - val_loss: 0.0643 - val_acc: 0.7376\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0646 - acc: 0.7395 - val_loss: 0.0668 - val_acc: 0.7286\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0646 - acc: 0.7404 - val_loss: 0.0654 - val_acc: 0.7349\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0644 - acc: 0.7404 - val_loss: 0.0706 - val_acc: 0.7279\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0644 - acc: 0.7403 - val_loss: 0.0640 - val_acc: 0.7455\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0643 - acc: 0.7413 - val_loss: 0.0638 - val_acc: 0.7460\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0643 - acc: 0.7416 - val_loss: 0.0642 - val_acc: 0.7483\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0642 - acc: 0.7409 - val_loss: 0.0644 - val_acc: 0.7435\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0641 - acc: 0.7419 - val_loss: 0.0648 - val_acc: 0.7400\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0640 - acc: 0.7418 - val_loss: 0.0635 - val_acc: 0.7480\n",
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0639 - acc: 0.7429 - val_loss: 0.0653 - val_acc: 0.7345\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0639 - acc: 0.7421 - val_loss: 0.0642 - val_acc: 0.7449\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0638 - acc: 0.7425 - val_loss: 0.0643 - val_acc: 0.7347\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0638 - acc: 0.7432 - val_loss: 0.0631 - val_acc: 0.7440\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0638 - acc: 0.7427 - val_loss: 0.0644 - val_acc: 0.7307\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0636 - acc: 0.7430 - val_loss: 0.0631 - val_acc: 0.7481\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0636 - acc: 0.7439 - val_loss: 0.0652 - val_acc: 0.7344\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0635 - acc: 0.7444 - val_loss: 0.0636 - val_acc: 0.7417\n",
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0635 - acc: 0.7436 - val_loss: 0.0636 - val_acc: 0.7458\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0635 - acc: 0.7448 - val_loss: 0.0631 - val_acc: 0.7492\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0634 - acc: 0.7450 - val_loss: 0.0640 - val_acc: 0.7357\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0633 - acc: 0.7447 - val_loss: 0.0636 - val_acc: 0.7407\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0633 - acc: 0.7451 - val_loss: 0.0639 - val_acc: 0.7461\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0632 - acc: 0.7446 - val_loss: 0.0635 - val_acc: 0.7428\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0632 - acc: 0.7444 - val_loss: 0.0620 - val_acc: 0.7486\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0631 - acc: 0.7459 - val_loss: 0.0635 - val_acc: 0.7435\n",
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0631 - acc: 0.7460 - val_loss: 0.0643 - val_acc: 0.7423\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 11s 59us/step - loss: 0.0631 - acc: 0.7461 - val_loss: 0.0628 - val_acc: 0.7473\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0630 - acc: 0.7465 - val_loss: 0.0627 - val_acc: 0.7518\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0629 - acc: 0.7459 - val_loss: 0.0631 - val_acc: 0.7438\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0629 - acc: 0.7471 - val_loss: 0.0633 - val_acc: 0.7490\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 8s 47us/step - loss: 0.0628 - acc: 0.7464 - val_loss: 0.0639 - val_acc: 0.7344\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0629 - acc: 0.7458 - val_loss: 0.0642 - val_acc: 0.7442\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0628 - acc: 0.7465 - val_loss: 0.0624 - val_acc: 0.7450\n",
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 8s 47us/step - loss: 0.0627 - acc: 0.7469 - val_loss: 0.0627 - val_acc: 0.7495\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0626 - acc: 0.7482 - val_loss: 0.0644 - val_acc: 0.7419\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0627 - acc: 0.7472 - val_loss: 0.0628 - val_acc: 0.7420\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 9s 50us/step - loss: 0.0626 - acc: 0.7478 - val_loss: 0.0631 - val_acc: 0.7445\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 8s 47us/step - loss: 0.0626 - acc: 0.7465 - val_loss: 0.0625 - val_acc: 0.7430\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0625 - acc: 0.7479 - val_loss: 0.0621 - val_acc: 0.7515\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0626 - acc: 0.7468 - val_loss: 0.0627 - val_acc: 0.7456\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0625 - acc: 0.7476 - val_loss: 0.0618 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0624 - acc: 0.7478 - val_loss: 0.0624 - val_acc: 0.7505\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 8s 47us/step - loss: 0.0624 - acc: 0.7473 - val_loss: 0.0620 - val_acc: 0.7484\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0624 - acc: 0.7467 - val_loss: 0.0624 - val_acc: 0.7474\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 8s 47us/step - loss: 0.0624 - acc: 0.7474 - val_loss: 0.0618 - val_acc: 0.7539\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0624 - acc: 0.7470 - val_loss: 0.0645 - val_acc: 0.7336\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0623 - acc: 0.7485 - val_loss: 0.0627 - val_acc: 0.7509\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0624 - acc: 0.7488 - val_loss: 0.0622 - val_acc: 0.7521\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0623 - acc: 0.7473 - val_loss: 0.0619 - val_acc: 0.7489\n",
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 8s 47us/step - loss: 0.0623 - acc: 0.7488 - val_loss: 0.0618 - val_acc: 0.7498\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0622 - acc: 0.7491 - val_loss: 0.0616 - val_acc: 0.7502\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.0622 - acc: 0.7489 - val_loss: 0.0615 - val_acc: 0.7518\n",
      "28303/28303 [==============================] - 1s 22us/step\n",
      "\n",
      "Loss: 0.0612338049603078\n",
      "\n",
      "Accuracy: 75.14397767058017%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  5  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n",
      "Train shapes:  (254731, 9) (254731, 5)\n",
      "Test shapes:  (28303, 9) (28303, 5)\n",
      "Train on 178311 samples, validate on 76420 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 9s 51us/step - loss: 0.1179 - acc: 0.5130 - val_loss: 0.0988 - val_acc: 0.5857\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0910 - acc: 0.6323 - val_loss: 0.0872 - val_acc: 0.6393\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 9s 50us/step - loss: 0.0844 - acc: 0.6633 - val_loss: 0.0822 - val_acc: 0.6718\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0809 - acc: 0.6799 - val_loss: 0.0809 - val_acc: 0.6529\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 9s 49us/step - loss: 0.0785 - acc: 0.6907 - val_loss: 0.0768 - val_acc: 0.7102\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0766 - acc: 0.7013 - val_loss: 0.0766 - val_acc: 0.7048\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0751 - acc: 0.7069 - val_loss: 0.0757 - val_acc: 0.6921\n",
      "Epoch 8/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0737 - acc: 0.7106 - val_loss: 0.0731 - val_acc: 0.7081\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0727 - acc: 0.7138 - val_loss: 0.0715 - val_acc: 0.7161\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0719 - acc: 0.7155 - val_loss: 0.0716 - val_acc: 0.7192\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0714 - acc: 0.7162 - val_loss: 0.0718 - val_acc: 0.7131\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0709 - acc: 0.7183 - val_loss: 0.0737 - val_acc: 0.6894\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0705 - acc: 0.7185 - val_loss: 0.0717 - val_acc: 0.7057\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0703 - acc: 0.7195 - val_loss: 0.0703 - val_acc: 0.7147\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0701 - acc: 0.7203 - val_loss: 0.0692 - val_acc: 0.7241\n",
      "Epoch 16/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0699 - acc: 0.7207 - val_loss: 0.0695 - val_acc: 0.7230\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0697 - acc: 0.7207 - val_loss: 0.0719 - val_acc: 0.7111\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0695 - acc: 0.7210 - val_loss: 0.0701 - val_acc: 0.7189\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0694 - acc: 0.7219 - val_loss: 0.0696 - val_acc: 0.7212\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0691 - acc: 0.7228 - val_loss: 0.0694 - val_acc: 0.7126\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0691 - acc: 0.7225 - val_loss: 0.0700 - val_acc: 0.7130\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0689 - acc: 0.7224 - val_loss: 0.0684 - val_acc: 0.7264\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0688 - acc: 0.7237 - val_loss: 0.0680 - val_acc: 0.7253\n",
      "Epoch 24/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0687 - acc: 0.7241 - val_loss: 0.0703 - val_acc: 0.7165\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0686 - acc: 0.7240 - val_loss: 0.0691 - val_acc: 0.7234\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0685 - acc: 0.7248 - val_loss: 0.0679 - val_acc: 0.7295\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0684 - acc: 0.7247 - val_loss: 0.0691 - val_acc: 0.7242\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0684 - acc: 0.7248 - val_loss: 0.0681 - val_acc: 0.7237\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0683 - acc: 0.7260 - val_loss: 0.0685 - val_acc: 0.7250\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0681 - acc: 0.7257 - val_loss: 0.0675 - val_acc: 0.7252\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0681 - acc: 0.7251 - val_loss: 0.0683 - val_acc: 0.7203\n",
      "Epoch 32/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0680 - acc: 0.7259 - val_loss: 0.0698 - val_acc: 0.7223\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0680 - acc: 0.7249 - val_loss: 0.0671 - val_acc: 0.7316\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0679 - acc: 0.7261 - val_loss: 0.0684 - val_acc: 0.7260\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0678 - acc: 0.7262 - val_loss: 0.0670 - val_acc: 0.7307\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0678 - acc: 0.7274 - val_loss: 0.0694 - val_acc: 0.7171\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0677 - acc: 0.7272 - val_loss: 0.0673 - val_acc: 0.7261\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0677 - acc: 0.7263 - val_loss: 0.0681 - val_acc: 0.7236\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0676 - acc: 0.7275 - val_loss: 0.0679 - val_acc: 0.7237\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0677 - acc: 0.7272 - val_loss: 0.0677 - val_acc: 0.7251\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0675 - acc: 0.7279 - val_loss: 0.0679 - val_acc: 0.7233\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0675 - acc: 0.7271 - val_loss: 0.0674 - val_acc: 0.7258\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0675 - acc: 0.7283 - val_loss: 0.0674 - val_acc: 0.7276\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0674 - acc: 0.7284 - val_loss: 0.0687 - val_acc: 0.7202\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0673 - acc: 0.7288 - val_loss: 0.0670 - val_acc: 0.7246\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0674 - acc: 0.7277 - val_loss: 0.0668 - val_acc: 0.7273\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0673 - acc: 0.7291 - val_loss: 0.0686 - val_acc: 0.7176\n",
      "Epoch 48/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0672 - acc: 0.7288 - val_loss: 0.0680 - val_acc: 0.7191\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 10s 59us/step - loss: 0.0672 - acc: 0.7290 - val_loss: 0.0669 - val_acc: 0.7329\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 10s 59us/step - loss: 0.0671 - acc: 0.7284 - val_loss: 0.0670 - val_acc: 0.7224\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0671 - acc: 0.7298 - val_loss: 0.0674 - val_acc: 0.7255\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0670 - acc: 0.7295 - val_loss: 0.0669 - val_acc: 0.7258\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0670 - acc: 0.7294 - val_loss: 0.0688 - val_acc: 0.7234\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0670 - acc: 0.7288 - val_loss: 0.0662 - val_acc: 0.7338\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0669 - acc: 0.7301 - val_loss: 0.0673 - val_acc: 0.7282\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 18s 101us/step - loss: 0.0669 - acc: 0.7296 - val_loss: 0.0663 - val_acc: 0.7323\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 19s 105us/step - loss: 0.0669 - acc: 0.7297 - val_loss: 0.0680 - val_acc: 0.7271\n",
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0669 - acc: 0.7301 - val_loss: 0.0682 - val_acc: 0.7240\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0668 - acc: 0.7298 - val_loss: 0.0663 - val_acc: 0.7300\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0668 - acc: 0.7306 - val_loss: 0.0661 - val_acc: 0.7333\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 11s 59us/step - loss: 0.0668 - acc: 0.7303 - val_loss: 0.0702 - val_acc: 0.7132\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0667 - acc: 0.7314 - val_loss: 0.0668 - val_acc: 0.7310\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0667 - acc: 0.7314 - val_loss: 0.0670 - val_acc: 0.7220\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0667 - acc: 0.7303 - val_loss: 0.0681 - val_acc: 0.7231\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0666 - acc: 0.7309 - val_loss: 0.0665 - val_acc: 0.7316\n",
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0666 - acc: 0.7308 - val_loss: 0.0663 - val_acc: 0.7300\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 10s 53us/step - loss: 0.0666 - acc: 0.7308 - val_loss: 0.0660 - val_acc: 0.7348\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0666 - acc: 0.7313 - val_loss: 0.0667 - val_acc: 0.7256\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0666 - acc: 0.7309 - val_loss: 0.0663 - val_acc: 0.7332\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0666 - acc: 0.7311 - val_loss: 0.0661 - val_acc: 0.7357\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0665 - acc: 0.7312 - val_loss: 0.0663 - val_acc: 0.7287\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0665 - acc: 0.7322 - val_loss: 0.0663 - val_acc: 0.7274\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0665 - acc: 0.7315 - val_loss: 0.0668 - val_acc: 0.7318\n",
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0664 - acc: 0.7325 - val_loss: 0.0662 - val_acc: 0.7319\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 9s 50us/step - loss: 0.0664 - acc: 0.7317 - val_loss: 0.0673 - val_acc: 0.7256\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0664 - acc: 0.7319 - val_loss: 0.0666 - val_acc: 0.7328\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0664 - acc: 0.7321 - val_loss: 0.0660 - val_acc: 0.7350\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0664 - acc: 0.7324 - val_loss: 0.0667 - val_acc: 0.7300\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0663 - acc: 0.7320 - val_loss: 0.0680 - val_acc: 0.7167\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0663 - acc: 0.7317 - val_loss: 0.0662 - val_acc: 0.7349\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0662 - acc: 0.7325 - val_loss: 0.0712 - val_acc: 0.6990\n",
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0662 - acc: 0.7324 - val_loss: 0.0661 - val_acc: 0.7300\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0663 - acc: 0.7328 - val_loss: 0.0675 - val_acc: 0.7226\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0663 - acc: 0.7318 - val_loss: 0.0671 - val_acc: 0.7298\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 13s 76us/step - loss: 0.0663 - acc: 0.7321 - val_loss: 0.0675 - val_acc: 0.7166\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0662 - acc: 0.7323 - val_loss: 0.0654 - val_acc: 0.7355\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0662 - acc: 0.7319 - val_loss: 0.0659 - val_acc: 0.7330\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0662 - acc: 0.7332 - val_loss: 0.0659 - val_acc: 0.7349\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0662 - acc: 0.7321 - val_loss: 0.0670 - val_acc: 0.7239\n",
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0661 - acc: 0.7332 - val_loss: 0.0658 - val_acc: 0.7327\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0661 - acc: 0.7324 - val_loss: 0.0660 - val_acc: 0.7263\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0661 - acc: 0.7333 - val_loss: 0.0657 - val_acc: 0.7336\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0660 - acc: 0.7339 - val_loss: 0.0658 - val_acc: 0.7303\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0661 - acc: 0.7320 - val_loss: 0.0659 - val_acc: 0.7348\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0661 - acc: 0.7335 - val_loss: 0.0672 - val_acc: 0.7257\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0660 - acc: 0.7325 - val_loss: 0.0664 - val_acc: 0.7343\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0660 - acc: 0.7330 - val_loss: 0.0662 - val_acc: 0.7331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0660 - acc: 0.7333 - val_loss: 0.0659 - val_acc: 0.7349\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0659 - acc: 0.7344 - val_loss: 0.0657 - val_acc: 0.7344\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0660 - acc: 0.7335 - val_loss: 0.0652 - val_acc: 0.7350\n",
      "28303/28303 [==============================] - 1s 24us/step\n",
      "\n",
      "Loss: 0.06532437463072135\n",
      "\n",
      "Accuracy: 73.42331201670991%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  6  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n",
      "Train shapes:  (254731, 9) (254731, 5)\n",
      "Test shapes:  (28303, 9) (28303, 5)\n",
      "Train on 178311 samples, validate on 76420 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 10s 53us/step - loss: 0.1174 - acc: 0.5164 - val_loss: 0.1000 - val_acc: 0.6088\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0950 - acc: 0.6037 - val_loss: 0.0916 - val_acc: 0.6319\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0892 - acc: 0.6319 - val_loss: 0.0867 - val_acc: 0.6478\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0851 - acc: 0.6521 - val_loss: 0.0833 - val_acc: 0.6559\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0823 - acc: 0.6671 - val_loss: 0.0810 - val_acc: 0.6709\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0801 - acc: 0.6800 - val_loss: 0.0789 - val_acc: 0.6811\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 9s 48us/step - loss: 0.0785 - acc: 0.6893 - val_loss: 0.0783 - val_acc: 0.6817\n",
      "Epoch 8/100\n",
      "178311/178311 [==============================] - 10s 53us/step - loss: 0.0771 - acc: 0.6967 - val_loss: 0.0774 - val_acc: 0.6937\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0759 - acc: 0.7026 - val_loss: 0.0763 - val_acc: 0.7036\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0750 - acc: 0.7052 - val_loss: 0.0746 - val_acc: 0.7071\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0743 - acc: 0.7077 - val_loss: 0.0743 - val_acc: 0.6976\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0737 - acc: 0.7102 - val_loss: 0.0765 - val_acc: 0.6864\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0732 - acc: 0.7111 - val_loss: 0.0738 - val_acc: 0.7082\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0729 - acc: 0.7115 - val_loss: 0.0724 - val_acc: 0.7105\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0725 - acc: 0.7131 - val_loss: 0.0724 - val_acc: 0.7047\n",
      "Epoch 16/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0723 - acc: 0.7138 - val_loss: 0.0723 - val_acc: 0.7076\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0720 - acc: 0.7145 - val_loss: 0.0719 - val_acc: 0.7190\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0718 - acc: 0.7153 - val_loss: 0.0720 - val_acc: 0.7116\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0716 - acc: 0.7156 - val_loss: 0.0717 - val_acc: 0.7106\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0714 - acc: 0.7174 - val_loss: 0.0715 - val_acc: 0.7107\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0712 - acc: 0.7168 - val_loss: 0.0703 - val_acc: 0.7199\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0710 - acc: 0.7179 - val_loss: 0.0715 - val_acc: 0.7184\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0709 - acc: 0.7170 - val_loss: 0.0704 - val_acc: 0.7213\n",
      "Epoch 24/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0706 - acc: 0.7189 - val_loss: 0.0704 - val_acc: 0.7127\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0705 - acc: 0.7192 - val_loss: 0.0713 - val_acc: 0.7110\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0703 - acc: 0.7187 - val_loss: 0.0695 - val_acc: 0.7250\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0701 - acc: 0.7206 - val_loss: 0.0700 - val_acc: 0.7200\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0699 - acc: 0.7211 - val_loss: 0.0695 - val_acc: 0.7277\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0697 - acc: 0.7221 - val_loss: 0.0697 - val_acc: 0.7201\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0695 - acc: 0.7222 - val_loss: 0.0701 - val_acc: 0.7119\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0694 - acc: 0.7220 - val_loss: 0.0693 - val_acc: 0.7253\n",
      "Epoch 32/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0693 - acc: 0.7226 - val_loss: 0.0690 - val_acc: 0.7205\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0691 - acc: 0.7235 - val_loss: 0.0687 - val_acc: 0.7235\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0690 - acc: 0.7242 - val_loss: 0.0681 - val_acc: 0.7271\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0688 - acc: 0.7245 - val_loss: 0.0683 - val_acc: 0.7292\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0686 - acc: 0.7253 - val_loss: 0.0680 - val_acc: 0.7247\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0685 - acc: 0.7260 - val_loss: 0.0685 - val_acc: 0.7225\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0684 - acc: 0.7272 - val_loss: 0.0674 - val_acc: 0.7290\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0683 - acc: 0.7271 - val_loss: 0.0673 - val_acc: 0.7268\n",
      "Epoch 40/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0682 - acc: 0.7259 - val_loss: 0.0689 - val_acc: 0.7265\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0680 - acc: 0.7271 - val_loss: 0.0671 - val_acc: 0.7292\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0680 - acc: 0.7273 - val_loss: 0.0671 - val_acc: 0.7304\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0678 - acc: 0.7277 - val_loss: 0.0668 - val_acc: 0.7346\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0677 - acc: 0.7279 - val_loss: 0.0679 - val_acc: 0.7249\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0675 - acc: 0.7297 - val_loss: 0.0678 - val_acc: 0.7353\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0674 - acc: 0.7292 - val_loss: 0.0672 - val_acc: 0.7361\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0673 - acc: 0.7293 - val_loss: 0.0675 - val_acc: 0.7248\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0673 - acc: 0.7301 - val_loss: 0.0675 - val_acc: 0.7281\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0671 - acc: 0.7300 - val_loss: 0.0665 - val_acc: 0.7286\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0671 - acc: 0.7297 - val_loss: 0.0666 - val_acc: 0.7263\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0670 - acc: 0.7312 - val_loss: 0.0670 - val_acc: 0.7313\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0669 - acc: 0.7314 - val_loss: 0.0668 - val_acc: 0.7268\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0669 - acc: 0.7316 - val_loss: 0.0665 - val_acc: 0.7379\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0667 - acc: 0.7325 - val_loss: 0.0666 - val_acc: 0.7356\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0667 - acc: 0.7321 - val_loss: 0.0658 - val_acc: 0.7345\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0666 - acc: 0.7324 - val_loss: 0.0670 - val_acc: 0.7296\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0666 - acc: 0.7321 - val_loss: 0.0667 - val_acc: 0.7279\n",
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0665 - acc: 0.7337 - val_loss: 0.0687 - val_acc: 0.7293\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0664 - acc: 0.7339 - val_loss: 0.0673 - val_acc: 0.7234\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0664 - acc: 0.7334 - val_loss: 0.0660 - val_acc: 0.7266\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0664 - acc: 0.7333 - val_loss: 0.0665 - val_acc: 0.7298\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0663 - acc: 0.7339 - val_loss: 0.0659 - val_acc: 0.7334\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0662 - acc: 0.7339 - val_loss: 0.0661 - val_acc: 0.7297\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0662 - acc: 0.7335 - val_loss: 0.0662 - val_acc: 0.7357\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0661 - acc: 0.7348 - val_loss: 0.0665 - val_acc: 0.7221\n",
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0660 - acc: 0.7348 - val_loss: 0.0658 - val_acc: 0.7267\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0659 - acc: 0.7357 - val_loss: 0.0654 - val_acc: 0.7312\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0659 - acc: 0.7360 - val_loss: 0.0660 - val_acc: 0.7281\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0658 - acc: 0.7359 - val_loss: 0.0654 - val_acc: 0.7327\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0658 - acc: 0.7366 - val_loss: 0.0651 - val_acc: 0.7400\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0657 - acc: 0.7367 - val_loss: 0.0649 - val_acc: 0.7413\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0657 - acc: 0.7369 - val_loss: 0.0653 - val_acc: 0.7414\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0656 - acc: 0.7369 - val_loss: 0.0655 - val_acc: 0.7454\n",
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0656 - acc: 0.7378 - val_loss: 0.0659 - val_acc: 0.7355\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0654 - acc: 0.7382 - val_loss: 0.0654 - val_acc: 0.7383\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0654 - acc: 0.7384 - val_loss: 0.0647 - val_acc: 0.7370\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0653 - acc: 0.7385 - val_loss: 0.0646 - val_acc: 0.7359\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0653 - acc: 0.7388 - val_loss: 0.0642 - val_acc: 0.7405\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0652 - acc: 0.7401 - val_loss: 0.0655 - val_acc: 0.7295\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 15s 84us/step - loss: 0.0652 - acc: 0.7396 - val_loss: 0.0654 - val_acc: 0.7382\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0650 - acc: 0.7403 - val_loss: 0.0650 - val_acc: 0.7415\n",
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0650 - acc: 0.7400 - val_loss: 0.0643 - val_acc: 0.7402\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0649 - acc: 0.7401 - val_loss: 0.0654 - val_acc: 0.7388\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0649 - acc: 0.7406 - val_loss: 0.0666 - val_acc: 0.7284\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0648 - acc: 0.7415 - val_loss: 0.0642 - val_acc: 0.7415\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0648 - acc: 0.7409 - val_loss: 0.0648 - val_acc: 0.7401\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0647 - acc: 0.7417 - val_loss: 0.0641 - val_acc: 0.7454\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0646 - acc: 0.7419 - val_loss: 0.0650 - val_acc: 0.7369\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0646 - acc: 0.7421 - val_loss: 0.0646 - val_acc: 0.7435\n",
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0646 - acc: 0.7422 - val_loss: 0.0640 - val_acc: 0.7394\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0646 - acc: 0.7425 - val_loss: 0.0652 - val_acc: 0.7399\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0645 - acc: 0.7425 - val_loss: 0.0642 - val_acc: 0.7414\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0644 - acc: 0.7437 - val_loss: 0.0637 - val_acc: 0.7429\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0644 - acc: 0.7430 - val_loss: 0.0635 - val_acc: 0.7481\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0643 - acc: 0.7436 - val_loss: 0.0636 - val_acc: 0.7461\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0643 - acc: 0.7434 - val_loss: 0.0642 - val_acc: 0.7426\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0642 - acc: 0.7443 - val_loss: 0.0653 - val_acc: 0.7452\n",
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0642 - acc: 0.7429 - val_loss: 0.0638 - val_acc: 0.7416\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0642 - acc: 0.7437 - val_loss: 0.0640 - val_acc: 0.7409\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0640 - acc: 0.7435 - val_loss: 0.0635 - val_acc: 0.7424\n",
      "28303/28303 [==============================] - 1s 42us/step\n",
      "\n",
      "Loss: 0.06320236865234295\n",
      "\n",
      "Accuracy: 74.56453379518834%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  7  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes:  (254731, 9) (254731, 5)\n",
      "Test shapes:  (28303, 9) (28303, 5)\n",
      "Train on 178311 samples, validate on 76420 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.1193 - acc: 0.5075 - val_loss: 0.0972 - val_acc: 0.5933\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0910 - acc: 0.6204 - val_loss: 0.0868 - val_acc: 0.6555\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0842 - acc: 0.6469 - val_loss: 0.0821 - val_acc: 0.6576\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0807 - acc: 0.6646 - val_loss: 0.0798 - val_acc: 0.6683\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0781 - acc: 0.6791 - val_loss: 0.0779 - val_acc: 0.6844\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0757 - acc: 0.6921 - val_loss: 0.0745 - val_acc: 0.6915\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0739 - acc: 0.7017 - val_loss: 0.0735 - val_acc: 0.7047\n",
      "Epoch 8/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0725 - acc: 0.7064 - val_loss: 0.0737 - val_acc: 0.6973\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0714 - acc: 0.7109 - val_loss: 0.0717 - val_acc: 0.7066\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 13s 76us/step - loss: 0.0707 - acc: 0.7135 - val_loss: 0.0727 - val_acc: 0.7090\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0701 - acc: 0.7159 - val_loss: 0.0712 - val_acc: 0.7052\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0696 - acc: 0.7167 - val_loss: 0.0689 - val_acc: 0.7175\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0689 - acc: 0.7185 - val_loss: 0.0689 - val_acc: 0.7214\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0684 - acc: 0.7204 - val_loss: 0.0694 - val_acc: 0.7123\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0679 - acc: 0.7224 - val_loss: 0.0681 - val_acc: 0.7224\n",
      "Epoch 16/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0674 - acc: 0.7234 - val_loss: 0.0666 - val_acc: 0.7278\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0669 - acc: 0.7251 - val_loss: 0.0681 - val_acc: 0.7082\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0666 - acc: 0.7262 - val_loss: 0.0671 - val_acc: 0.7210\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0663 - acc: 0.7278 - val_loss: 0.0669 - val_acc: 0.7310\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0660 - acc: 0.7281 - val_loss: 0.0679 - val_acc: 0.7137\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0656 - acc: 0.7288 - val_loss: 0.0660 - val_acc: 0.7313\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0655 - acc: 0.7309 - val_loss: 0.0650 - val_acc: 0.7277\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0653 - acc: 0.7310 - val_loss: 0.0655 - val_acc: 0.7288\n",
      "Epoch 24/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0652 - acc: 0.7308 - val_loss: 0.0650 - val_acc: 0.7313\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0650 - acc: 0.7320 - val_loss: 0.0656 - val_acc: 0.7252\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0649 - acc: 0.7330 - val_loss: 0.0676 - val_acc: 0.7021\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0646 - acc: 0.7335 - val_loss: 0.0653 - val_acc: 0.7339\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0646 - acc: 0.7346 - val_loss: 0.0657 - val_acc: 0.7366\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0644 - acc: 0.7351 - val_loss: 0.0649 - val_acc: 0.7361\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0644 - acc: 0.7357 - val_loss: 0.0644 - val_acc: 0.7408\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0642 - acc: 0.7356 - val_loss: 0.0650 - val_acc: 0.7281\n",
      "Epoch 32/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0641 - acc: 0.7359 - val_loss: 0.0638 - val_acc: 0.7406\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0640 - acc: 0.7369 - val_loss: 0.0642 - val_acc: 0.7403\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0640 - acc: 0.7369 - val_loss: 0.0647 - val_acc: 0.7303\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0638 - acc: 0.7373 - val_loss: 0.0639 - val_acc: 0.7339\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0637 - acc: 0.7382 - val_loss: 0.0649 - val_acc: 0.7267\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0636 - acc: 0.7396 - val_loss: 0.0632 - val_acc: 0.7450\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0636 - acc: 0.7386 - val_loss: 0.0634 - val_acc: 0.7367\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0634 - acc: 0.7401 - val_loss: 0.0641 - val_acc: 0.7292\n",
      "Epoch 40/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0634 - acc: 0.7393 - val_loss: 0.0636 - val_acc: 0.7362\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0633 - acc: 0.7402 - val_loss: 0.0646 - val_acc: 0.7296\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0632 - acc: 0.7403 - val_loss: 0.0633 - val_acc: 0.7417\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0631 - acc: 0.7402 - val_loss: 0.0652 - val_acc: 0.7280\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0630 - acc: 0.7407 - val_loss: 0.0639 - val_acc: 0.7394\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0631 - acc: 0.7403 - val_loss: 0.0646 - val_acc: 0.7352\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0630 - acc: 0.7413 - val_loss: 0.0642 - val_acc: 0.7375\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0629 - acc: 0.7422 - val_loss: 0.0643 - val_acc: 0.7379\n",
      "Epoch 48/100\n",
      "178311/178311 [==============================] - 18s 99us/step - loss: 0.0629 - acc: 0.7411 - val_loss: 0.0715 - val_acc: 0.7128\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0628 - acc: 0.7422 - val_loss: 0.0632 - val_acc: 0.7384\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0627 - acc: 0.7415 - val_loss: 0.0641 - val_acc: 0.7410\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0627 - acc: 0.7422 - val_loss: 0.0632 - val_acc: 0.7347\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0626 - acc: 0.7424 - val_loss: 0.0638 - val_acc: 0.7346\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0626 - acc: 0.7429 - val_loss: 0.0625 - val_acc: 0.7412\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0625 - acc: 0.7429 - val_loss: 0.0630 - val_acc: 0.7477\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0625 - acc: 0.7433 - val_loss: 0.0633 - val_acc: 0.7374\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0624 - acc: 0.7435 - val_loss: 0.0632 - val_acc: 0.7374\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0623 - acc: 0.7434 - val_loss: 0.0625 - val_acc: 0.7419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0623 - acc: 0.7444 - val_loss: 0.0634 - val_acc: 0.7407\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0622 - acc: 0.7442 - val_loss: 0.0625 - val_acc: 0.7406\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0622 - acc: 0.7434 - val_loss: 0.0639 - val_acc: 0.7467\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0622 - acc: 0.7444 - val_loss: 0.0631 - val_acc: 0.7404\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0621 - acc: 0.7449 - val_loss: 0.0615 - val_acc: 0.7493\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0621 - acc: 0.7445 - val_loss: 0.0629 - val_acc: 0.7448\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 21s 116us/step - loss: 0.0621 - acc: 0.7451 - val_loss: 0.0630 - val_acc: 0.7297\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0621 - acc: 0.7451 - val_loss: 0.0623 - val_acc: 0.7478\n",
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0619 - acc: 0.7453 - val_loss: 0.0621 - val_acc: 0.7452\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0619 - acc: 0.7456 - val_loss: 0.0628 - val_acc: 0.7413\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0618 - acc: 0.7457 - val_loss: 0.0618 - val_acc: 0.7454\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 10s 59us/step - loss: 0.0619 - acc: 0.7462 - val_loss: 0.0618 - val_acc: 0.7503\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0618 - acc: 0.7464 - val_loss: 0.0627 - val_acc: 0.7463\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0618 - acc: 0.7457 - val_loss: 0.0615 - val_acc: 0.7469\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0618 - acc: 0.7462 - val_loss: 0.0619 - val_acc: 0.7496\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0618 - acc: 0.7463 - val_loss: 0.0627 - val_acc: 0.7424\n",
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0617 - acc: 0.7465 - val_loss: 0.0633 - val_acc: 0.7408\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0618 - acc: 0.7460 - val_loss: 0.0612 - val_acc: 0.7483\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0616 - acc: 0.7466 - val_loss: 0.0628 - val_acc: 0.7434\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0616 - acc: 0.7464 - val_loss: 0.0618 - val_acc: 0.7463\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0616 - acc: 0.7471 - val_loss: 0.0616 - val_acc: 0.7471\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0615 - acc: 0.7468 - val_loss: 0.0611 - val_acc: 0.7478\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0615 - acc: 0.7478 - val_loss: 0.0619 - val_acc: 0.7485\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0614 - acc: 0.7469 - val_loss: 0.0610 - val_acc: 0.7523\n",
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0614 - acc: 0.7461 - val_loss: 0.0615 - val_acc: 0.7432\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0613 - acc: 0.7475 - val_loss: 0.0609 - val_acc: 0.7480\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0613 - acc: 0.7474 - val_loss: 0.0617 - val_acc: 0.7483\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0613 - acc: 0.7467 - val_loss: 0.0620 - val_acc: 0.7474\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0613 - acc: 0.7478 - val_loss: 0.0608 - val_acc: 0.7502\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0612 - acc: 0.7485 - val_loss: 0.0609 - val_acc: 0.7477\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 17s 94us/step - loss: 0.0612 - acc: 0.7475 - val_loss: 0.0621 - val_acc: 0.7439\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 20s 112us/step - loss: 0.0611 - acc: 0.7482 - val_loss: 0.0617 - val_acc: 0.7483\n",
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0611 - acc: 0.7483 - val_loss: 0.0613 - val_acc: 0.7487\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0610 - acc: 0.7486 - val_loss: 0.0614 - val_acc: 0.7478\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0610 - acc: 0.7487 - val_loss: 0.0609 - val_acc: 0.7512\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0610 - acc: 0.7490 - val_loss: 0.0615 - val_acc: 0.7442\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0610 - acc: 0.7490 - val_loss: 0.0608 - val_acc: 0.7493\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0609 - acc: 0.7494 - val_loss: 0.0620 - val_acc: 0.7432\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0609 - acc: 0.7484 - val_loss: 0.0630 - val_acc: 0.7316\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0608 - acc: 0.7487 - val_loss: 0.0614 - val_acc: 0.7484\n",
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0609 - acc: 0.7489 - val_loss: 0.0611 - val_acc: 0.7453\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0608 - acc: 0.7488 - val_loss: 0.0627 - val_acc: 0.7405\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0608 - acc: 0.7503 - val_loss: 0.0607 - val_acc: 0.7508\n",
      "28303/28303 [==============================] - 1s 33us/step\n",
      "\n",
      "Loss: 0.060013586100144414\n",
      "\n",
      "Accuracy: 75.75522029503695%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  8  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n",
      "Train shapes:  (254731, 9) (254731, 5)\n",
      "Test shapes:  (28303, 9) (28303, 5)\n",
      "Train on 178311 samples, validate on 76420 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 13s 76us/step - loss: 0.1249 - acc: 0.4810 - val_loss: 0.1081 - val_acc: 0.5926\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 16s 92us/step - loss: 0.0978 - acc: 0.6174 - val_loss: 0.0921 - val_acc: 0.6282\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 19s 105us/step - loss: 0.0896 - acc: 0.6390 - val_loss: 0.0878 - val_acc: 0.6522\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0846 - acc: 0.6657 - val_loss: 0.0829 - val_acc: 0.6837\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0800 - acc: 0.6942 - val_loss: 0.0790 - val_acc: 0.6967\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0768 - acc: 0.7041 - val_loss: 0.0772 - val_acc: 0.6951\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0747 - acc: 0.7105 - val_loss: 0.0745 - val_acc: 0.7163\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0735 - acc: 0.7133 - val_loss: 0.0739 - val_acc: 0.7090\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0726 - acc: 0.7171 - val_loss: 0.0728 - val_acc: 0.7179\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0721 - acc: 0.7182 - val_loss: 0.0720 - val_acc: 0.7192\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.0715 - acc: 0.7195 - val_loss: 0.0726 - val_acc: 0.7120\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 13s 74us/step - loss: 0.0710 - acc: 0.7205 - val_loss: 0.0728 - val_acc: 0.7019\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0704 - acc: 0.7210 - val_loss: 0.0704 - val_acc: 0.7187\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0700 - acc: 0.7227 - val_loss: 0.0704 - val_acc: 0.7211\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 13s 75us/step - loss: 0.0697 - acc: 0.7228 - val_loss: 0.0705 - val_acc: 0.7150\n",
      "Epoch 16/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0694 - acc: 0.7236 - val_loss: 0.0703 - val_acc: 0.7186\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0691 - acc: 0.7236 - val_loss: 0.0687 - val_acc: 0.7279\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0690 - acc: 0.7242 - val_loss: 0.0708 - val_acc: 0.7179\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0687 - acc: 0.7256 - val_loss: 0.0694 - val_acc: 0.7189\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0686 - acc: 0.7238 - val_loss: 0.0694 - val_acc: 0.7194\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 18s 100us/step - loss: 0.0684 - acc: 0.7248 - val_loss: 0.0687 - val_acc: 0.7301\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0683 - acc: 0.7252 - val_loss: 0.0740 - val_acc: 0.6869\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 15s 82us/step - loss: 0.0681 - acc: 0.7260 - val_loss: 0.0677 - val_acc: 0.7258\n",
      "Epoch 24/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0679 - acc: 0.7267 - val_loss: 0.0675 - val_acc: 0.7246\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0678 - acc: 0.7265 - val_loss: 0.0682 - val_acc: 0.7227\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0677 - acc: 0.7272 - val_loss: 0.0677 - val_acc: 0.7250\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 17s 98us/step - loss: 0.0676 - acc: 0.7280 - val_loss: 0.0693 - val_acc: 0.7192\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0674 - acc: 0.7274 - val_loss: 0.0677 - val_acc: 0.7181\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0673 - acc: 0.7289 - val_loss: 0.0689 - val_acc: 0.7120\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0672 - acc: 0.7295 - val_loss: 0.0676 - val_acc: 0.7158\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 18s 98us/step - loss: 0.0670 - acc: 0.7299 - val_loss: 0.0677 - val_acc: 0.7251\n",
      "Epoch 32/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0668 - acc: 0.7313 - val_loss: 0.0682 - val_acc: 0.7257\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0668 - acc: 0.7295 - val_loss: 0.0668 - val_acc: 0.7300\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0666 - acc: 0.7320 - val_loss: 0.0692 - val_acc: 0.7162\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0665 - acc: 0.7320 - val_loss: 0.0675 - val_acc: 0.7261\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0664 - acc: 0.7322 - val_loss: 0.0676 - val_acc: 0.7188\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0663 - acc: 0.7327 - val_loss: 0.0658 - val_acc: 0.7330\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0661 - acc: 0.7334 - val_loss: 0.0687 - val_acc: 0.7217\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0660 - acc: 0.7342 - val_loss: 0.0666 - val_acc: 0.7331\n",
      "Epoch 40/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0659 - acc: 0.7355 - val_loss: 0.0673 - val_acc: 0.7252\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0658 - acc: 0.7355 - val_loss: 0.0662 - val_acc: 0.7322\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0657 - acc: 0.7359 - val_loss: 0.0663 - val_acc: 0.7311\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0656 - acc: 0.7362 - val_loss: 0.0675 - val_acc: 0.7230\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0655 - acc: 0.7365 - val_loss: 0.0653 - val_acc: 0.7373\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0653 - acc: 0.7367 - val_loss: 0.0659 - val_acc: 0.7404\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0652 - acc: 0.7372 - val_loss: 0.0657 - val_acc: 0.7351\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0651 - acc: 0.7378 - val_loss: 0.0646 - val_acc: 0.7418\n",
      "Epoch 48/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0649 - acc: 0.7377 - val_loss: 0.0654 - val_acc: 0.7398\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0648 - acc: 0.7392 - val_loss: 0.0652 - val_acc: 0.7388\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0647 - acc: 0.7391 - val_loss: 0.0657 - val_acc: 0.7321\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0646 - acc: 0.7393 - val_loss: 0.0654 - val_acc: 0.7352\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0644 - acc: 0.7403 - val_loss: 0.0639 - val_acc: 0.7415\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0641 - acc: 0.7415 - val_loss: 0.0639 - val_acc: 0.7429\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0640 - acc: 0.7421 - val_loss: 0.0640 - val_acc: 0.7364\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0639 - acc: 0.7426 - val_loss: 0.0643 - val_acc: 0.7404\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0637 - acc: 0.7428 - val_loss: 0.0647 - val_acc: 0.7387\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0636 - acc: 0.7431 - val_loss: 0.0653 - val_acc: 0.7237\n",
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0636 - acc: 0.7430 - val_loss: 0.0651 - val_acc: 0.7387\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0635 - acc: 0.7438 - val_loss: 0.0629 - val_acc: 0.7472\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0634 - acc: 0.7439 - val_loss: 0.0634 - val_acc: 0.7427\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0634 - acc: 0.7442 - val_loss: 0.0632 - val_acc: 0.7452\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0633 - acc: 0.7452 - val_loss: 0.0638 - val_acc: 0.7414\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0632 - acc: 0.7447 - val_loss: 0.0663 - val_acc: 0.7296\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 9s 53us/step - loss: 0.0633 - acc: 0.7440 - val_loss: 0.0648 - val_acc: 0.7406\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0632 - acc: 0.7445 - val_loss: 0.0639 - val_acc: 0.7397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0632 - acc: 0.7444 - val_loss: 0.0635 - val_acc: 0.7414\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 10s 53us/step - loss: 0.0631 - acc: 0.7449 - val_loss: 0.0656 - val_acc: 0.7284\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 9s 52us/step - loss: 0.0631 - acc: 0.7451 - val_loss: 0.0640 - val_acc: 0.7402\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0631 - acc: 0.7452 - val_loss: 0.0639 - val_acc: 0.7405\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0631 - acc: 0.7452 - val_loss: 0.0636 - val_acc: 0.7477\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0630 - acc: 0.7457 - val_loss: 0.0673 - val_acc: 0.7201\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0630 - acc: 0.7455 - val_loss: 0.0632 - val_acc: 0.7455\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0629 - acc: 0.7460 - val_loss: 0.0630 - val_acc: 0.7471\n",
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 9s 52us/step - loss: 0.0629 - acc: 0.7465 - val_loss: 0.0629 - val_acc: 0.7482\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0628 - acc: 0.7464 - val_loss: 0.0635 - val_acc: 0.7455\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 10s 57us/step - loss: 0.0628 - acc: 0.7465 - val_loss: 0.0628 - val_acc: 0.7426\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0627 - acc: 0.7474 - val_loss: 0.0639 - val_acc: 0.7440\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0628 - acc: 0.7467 - val_loss: 0.0631 - val_acc: 0.7469\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 10s 55us/step - loss: 0.0627 - acc: 0.7479 - val_loss: 0.0627 - val_acc: 0.7467\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0627 - acc: 0.7461 - val_loss: 0.0628 - val_acc: 0.7472\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0627 - acc: 0.7478 - val_loss: 0.0632 - val_acc: 0.7398\n",
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0627 - acc: 0.7466 - val_loss: 0.0641 - val_acc: 0.7392\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0626 - acc: 0.7477 - val_loss: 0.0627 - val_acc: 0.7427\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0626 - acc: 0.7481 - val_loss: 0.0627 - val_acc: 0.7478\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 11s 59us/step - loss: 0.0626 - acc: 0.7477 - val_loss: 0.0625 - val_acc: 0.7437\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0626 - acc: 0.7473 - val_loss: 0.0633 - val_acc: 0.7471\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0625 - acc: 0.7483 - val_loss: 0.0629 - val_acc: 0.7509\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0626 - acc: 0.7481 - val_loss: 0.0636 - val_acc: 0.7477\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0625 - acc: 0.7484 - val_loss: 0.0624 - val_acc: 0.7486\n",
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0625 - acc: 0.7487 - val_loss: 0.0629 - val_acc: 0.7419\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0625 - acc: 0.7482 - val_loss: 0.0631 - val_acc: 0.7442\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0624 - acc: 0.7489 - val_loss: 0.0628 - val_acc: 0.7475\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0625 - acc: 0.7481 - val_loss: 0.0626 - val_acc: 0.7455\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0624 - acc: 0.7483 - val_loss: 0.0648 - val_acc: 0.7335\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0625 - acc: 0.7485 - val_loss: 0.0626 - val_acc: 0.7476\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0624 - acc: 0.7486 - val_loss: 0.0629 - val_acc: 0.7469\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 10s 54us/step - loss: 0.0623 - acc: 0.7487 - val_loss: 0.0628 - val_acc: 0.7438\n",
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 10s 59us/step - loss: 0.0624 - acc: 0.7483 - val_loss: 0.0635 - val_acc: 0.7430\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0623 - acc: 0.7483 - val_loss: 0.0622 - val_acc: 0.7497\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0623 - acc: 0.7487 - val_loss: 0.0635 - val_acc: 0.7419\n",
      "28303/28303 [==============================] - 1s 31us/step\n",
      "\n",
      "Loss: 0.06417451622437426\n",
      "\n",
      "Accuracy: 74.11935130567326%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  9  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n",
      "Train shapes:  (254731, 9) (254731, 5)\n",
      "Test shapes:  (28303, 9) (28303, 5)\n",
      "Train on 178311 samples, validate on 76420 samples\n",
      "Epoch 1/100\n",
      "178311/178311 [==============================] - 12s 70us/step - loss: 0.1199 - acc: 0.5244 - val_loss: 0.1033 - val_acc: 0.5898\n",
      "Epoch 2/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0968 - acc: 0.5990 - val_loss: 0.0925 - val_acc: 0.6096\n",
      "Epoch 3/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0897 - acc: 0.6188 - val_loss: 0.0887 - val_acc: 0.6016\n",
      "Epoch 4/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0864 - acc: 0.6368 - val_loss: 0.0852 - val_acc: 0.6256\n",
      "Epoch 5/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0835 - acc: 0.6513 - val_loss: 0.0816 - val_acc: 0.6745\n",
      "Epoch 6/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0811 - acc: 0.6709 - val_loss: 0.0795 - val_acc: 0.6960\n",
      "Epoch 7/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0787 - acc: 0.6911 - val_loss: 0.0786 - val_acc: 0.6951\n",
      "Epoch 8/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0768 - acc: 0.7013 - val_loss: 0.0770 - val_acc: 0.7079\n",
      "Epoch 9/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0754 - acc: 0.7068 - val_loss: 0.0749 - val_acc: 0.7166\n",
      "Epoch 10/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0746 - acc: 0.7088 - val_loss: 0.0774 - val_acc: 0.6608\n",
      "Epoch 11/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0737 - acc: 0.7118 - val_loss: 0.0727 - val_acc: 0.7125\n",
      "Epoch 12/100\n",
      "178311/178311 [==============================] - 10s 58us/step - loss: 0.0731 - acc: 0.7146 - val_loss: 0.0743 - val_acc: 0.7039\n",
      "Epoch 13/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0724 - acc: 0.7158 - val_loss: 0.0730 - val_acc: 0.7129\n",
      "Epoch 14/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0718 - acc: 0.7171 - val_loss: 0.0721 - val_acc: 0.7124\n",
      "Epoch 15/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0713 - acc: 0.7190 - val_loss: 0.0711 - val_acc: 0.7217\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0709 - acc: 0.7204 - val_loss: 0.0714 - val_acc: 0.7204\n",
      "Epoch 17/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0706 - acc: 0.7207 - val_loss: 0.0705 - val_acc: 0.7204\n",
      "Epoch 18/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0702 - acc: 0.7230 - val_loss: 0.0697 - val_acc: 0.7274\n",
      "Epoch 19/100\n",
      "178311/178311 [==============================] - 15s 87us/step - loss: 0.0699 - acc: 0.7248 - val_loss: 0.0700 - val_acc: 0.7146\n",
      "Epoch 20/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0697 - acc: 0.7240 - val_loss: 0.0697 - val_acc: 0.7263\n",
      "Epoch 21/100\n",
      "178311/178311 [==============================] - 17s 93us/step - loss: 0.0693 - acc: 0.7251 - val_loss: 0.0702 - val_acc: 0.7185\n",
      "Epoch 22/100\n",
      "178311/178311 [==============================] - 16s 87us/step - loss: 0.0691 - acc: 0.7261 - val_loss: 0.0713 - val_acc: 0.7202\n",
      "Epoch 23/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0688 - acc: 0.7275 - val_loss: 0.0685 - val_acc: 0.7343\n",
      "Epoch 24/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0685 - acc: 0.7280 - val_loss: 0.0686 - val_acc: 0.7295\n",
      "Epoch 25/100\n",
      "178311/178311 [==============================] - 14s 79us/step - loss: 0.0681 - acc: 0.7289 - val_loss: 0.0696 - val_acc: 0.7169\n",
      "Epoch 26/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0678 - acc: 0.7300 - val_loss: 0.0671 - val_acc: 0.7332\n",
      "Epoch 27/100\n",
      "178311/178311 [==============================] - 14s 78us/step - loss: 0.0675 - acc: 0.7314 - val_loss: 0.0672 - val_acc: 0.7342\n",
      "Epoch 28/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0673 - acc: 0.7319 - val_loss: 0.0665 - val_acc: 0.7354\n",
      "Epoch 29/100\n",
      "178311/178311 [==============================] - 16s 88us/step - loss: 0.0671 - acc: 0.7317 - val_loss: 0.0664 - val_acc: 0.7375\n",
      "Epoch 30/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0668 - acc: 0.7332 - val_loss: 0.0681 - val_acc: 0.7227\n",
      "Epoch 31/100\n",
      "178311/178311 [==============================] - 16s 89us/step - loss: 0.0666 - acc: 0.7344 - val_loss: 0.0668 - val_acc: 0.7351\n",
      "Epoch 32/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0665 - acc: 0.7347 - val_loss: 0.0664 - val_acc: 0.7357\n",
      "Epoch 33/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0662 - acc: 0.7358 - val_loss: 0.0672 - val_acc: 0.7218\n",
      "Epoch 34/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0661 - acc: 0.7367 - val_loss: 0.0659 - val_acc: 0.7408\n",
      "Epoch 35/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0659 - acc: 0.7368 - val_loss: 0.0653 - val_acc: 0.7447\n",
      "Epoch 36/100\n",
      "178311/178311 [==============================] - 10s 59us/step - loss: 0.0658 - acc: 0.7371 - val_loss: 0.0655 - val_acc: 0.7399\n",
      "Epoch 37/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0656 - acc: 0.7371 - val_loss: 0.0664 - val_acc: 0.7362\n",
      "Epoch 38/100\n",
      "178311/178311 [==============================] - 11s 59us/step - loss: 0.0655 - acc: 0.7380 - val_loss: 0.0658 - val_acc: 0.7361\n",
      "Epoch 39/100\n",
      "178311/178311 [==============================] - 10s 59us/step - loss: 0.0654 - acc: 0.7386 - val_loss: 0.0654 - val_acc: 0.7303\n",
      "Epoch 40/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0652 - acc: 0.7390 - val_loss: 0.0654 - val_acc: 0.7389\n",
      "Epoch 41/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0651 - acc: 0.7388 - val_loss: 0.0649 - val_acc: 0.7346\n",
      "Epoch 42/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0650 - acc: 0.7388 - val_loss: 0.0652 - val_acc: 0.7455\n",
      "Epoch 43/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0650 - acc: 0.7381 - val_loss: 0.0646 - val_acc: 0.7390\n",
      "Epoch 44/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0649 - acc: 0.7388 - val_loss: 0.0643 - val_acc: 0.7402\n",
      "Epoch 45/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0648 - acc: 0.7400 - val_loss: 0.0639 - val_acc: 0.7442\n",
      "Epoch 46/100\n",
      "178311/178311 [==============================] - 14s 77us/step - loss: 0.0646 - acc: 0.7408 - val_loss: 0.0640 - val_acc: 0.7469\n",
      "Epoch 47/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0646 - acc: 0.7407 - val_loss: 0.0641 - val_acc: 0.7441\n",
      "Epoch 48/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0645 - acc: 0.7415 - val_loss: 0.0640 - val_acc: 0.7439\n",
      "Epoch 49/100\n",
      "178311/178311 [==============================] - 13s 72us/step - loss: 0.0644 - acc: 0.7415 - val_loss: 0.0654 - val_acc: 0.7374\n",
      "Epoch 50/100\n",
      "178311/178311 [==============================] - 15s 83us/step - loss: 0.0644 - acc: 0.7417 - val_loss: 0.0647 - val_acc: 0.7451\n",
      "Epoch 51/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0644 - acc: 0.7408 - val_loss: 0.0641 - val_acc: 0.7400\n",
      "Epoch 52/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0642 - acc: 0.7411 - val_loss: 0.0645 - val_acc: 0.7371\n",
      "Epoch 53/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0642 - acc: 0.7411 - val_loss: 0.0643 - val_acc: 0.7428\n",
      "Epoch 54/100\n",
      "178311/178311 [==============================] - 12s 69us/step - loss: 0.0641 - acc: 0.7424 - val_loss: 0.0640 - val_acc: 0.7421\n",
      "Epoch 55/100\n",
      "178311/178311 [==============================] - 13s 71us/step - loss: 0.0640 - acc: 0.7435 - val_loss: 0.0640 - val_acc: 0.7409\n",
      "Epoch 56/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0639 - acc: 0.7436 - val_loss: 0.0635 - val_acc: 0.7451\n",
      "Epoch 57/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0639 - acc: 0.7424 - val_loss: 0.0643 - val_acc: 0.7408\n",
      "Epoch 58/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0638 - acc: 0.7439 - val_loss: 0.0638 - val_acc: 0.7447\n",
      "Epoch 59/100\n",
      "178311/178311 [==============================] - 13s 70us/step - loss: 0.0638 - acc: 0.7431 - val_loss: 0.0633 - val_acc: 0.7434\n",
      "Epoch 60/100\n",
      "178311/178311 [==============================] - 15s 86us/step - loss: 0.0637 - acc: 0.7434 - val_loss: 0.0624 - val_acc: 0.7507\n",
      "Epoch 61/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0636 - acc: 0.7439 - val_loss: 0.0631 - val_acc: 0.7454\n",
      "Epoch 62/100\n",
      "178311/178311 [==============================] - 14s 80us/step - loss: 0.0636 - acc: 0.7431 - val_loss: 0.0631 - val_acc: 0.7397\n",
      "Epoch 63/100\n",
      "178311/178311 [==============================] - 15s 85us/step - loss: 0.0636 - acc: 0.7436 - val_loss: 0.0634 - val_acc: 0.7458\n",
      "Epoch 64/100\n",
      "178311/178311 [==============================] - 16s 91us/step - loss: 0.0635 - acc: 0.7431 - val_loss: 0.0628 - val_acc: 0.7488\n",
      "Epoch 65/100\n",
      "178311/178311 [==============================] - 17s 97us/step - loss: 0.0635 - acc: 0.7446 - val_loss: 0.0650 - val_acc: 0.7417\n",
      "Epoch 66/100\n",
      "178311/178311 [==============================] - 16s 88us/step - loss: 0.0635 - acc: 0.7433 - val_loss: 0.0626 - val_acc: 0.7495\n",
      "Epoch 67/100\n",
      "178311/178311 [==============================] - 13s 73us/step - loss: 0.0635 - acc: 0.7439 - val_loss: 0.0632 - val_acc: 0.7436\n",
      "Epoch 68/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0634 - acc: 0.7434 - val_loss: 0.0639 - val_acc: 0.7503\n",
      "Epoch 69/100\n",
      "178311/178311 [==============================] - 11s 63us/step - loss: 0.0633 - acc: 0.7445 - val_loss: 0.0641 - val_acc: 0.7416\n",
      "Epoch 70/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0633 - acc: 0.7441 - val_loss: 0.0627 - val_acc: 0.7481\n",
      "Epoch 71/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0632 - acc: 0.7451 - val_loss: 0.0643 - val_acc: 0.7369\n",
      "Epoch 72/100\n",
      "178311/178311 [==============================] - 12s 68us/step - loss: 0.0632 - acc: 0.7452 - val_loss: 0.0637 - val_acc: 0.7440\n",
      "Epoch 73/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0632 - acc: 0.7451 - val_loss: 0.0633 - val_acc: 0.7459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0631 - acc: 0.7446 - val_loss: 0.0632 - val_acc: 0.7490\n",
      "Epoch 75/100\n",
      "178311/178311 [==============================] - 14s 76us/step - loss: 0.0631 - acc: 0.7454 - val_loss: 0.0628 - val_acc: 0.7502\n",
      "Epoch 76/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0631 - acc: 0.7449 - val_loss: 0.0635 - val_acc: 0.7506\n",
      "Epoch 77/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0631 - acc: 0.7455 - val_loss: 0.0626 - val_acc: 0.7502\n",
      "Epoch 78/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0630 - acc: 0.7457 - val_loss: 0.0623 - val_acc: 0.7517\n",
      "Epoch 79/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0630 - acc: 0.7452 - val_loss: 0.0632 - val_acc: 0.7501\n",
      "Epoch 80/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0629 - acc: 0.7457 - val_loss: 0.0643 - val_acc: 0.7445\n",
      "Epoch 81/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0630 - acc: 0.7451 - val_loss: 0.0631 - val_acc: 0.7369\n",
      "Epoch 82/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0629 - acc: 0.7461 - val_loss: 0.0630 - val_acc: 0.7442\n",
      "Epoch 83/100\n",
      "178311/178311 [==============================] - 12s 65us/step - loss: 0.0630 - acc: 0.7450 - val_loss: 0.0631 - val_acc: 0.7421\n",
      "Epoch 84/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0629 - acc: 0.7450 - val_loss: 0.0632 - val_acc: 0.7449\n",
      "Epoch 85/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0629 - acc: 0.7453 - val_loss: 0.0635 - val_acc: 0.7336\n",
      "Epoch 86/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0628 - acc: 0.7450 - val_loss: 0.0636 - val_acc: 0.7465\n",
      "Epoch 87/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0628 - acc: 0.7466 - val_loss: 0.0625 - val_acc: 0.7533\n",
      "Epoch 88/100\n",
      "178311/178311 [==============================] - 11s 61us/step - loss: 0.0628 - acc: 0.7452 - val_loss: 0.0629 - val_acc: 0.7474\n",
      "Epoch 89/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0627 - acc: 0.7466 - val_loss: 0.0634 - val_acc: 0.7489\n",
      "Epoch 90/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0627 - acc: 0.7465 - val_loss: 0.0629 - val_acc: 0.7490\n",
      "Epoch 91/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0626 - acc: 0.7455 - val_loss: 0.0624 - val_acc: 0.7455\n",
      "Epoch 92/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0626 - acc: 0.7461 - val_loss: 0.0630 - val_acc: 0.7480\n",
      "Epoch 93/100\n",
      "178311/178311 [==============================] - 11s 64us/step - loss: 0.0626 - acc: 0.7471 - val_loss: 0.0621 - val_acc: 0.7492\n",
      "Epoch 94/100\n",
      "178311/178311 [==============================] - 12s 67us/step - loss: 0.0625 - acc: 0.7464 - val_loss: 0.0632 - val_acc: 0.7489\n",
      "Epoch 95/100\n",
      "178311/178311 [==============================] - 11s 59us/step - loss: 0.0626 - acc: 0.7472 - val_loss: 0.0633 - val_acc: 0.7506\n",
      "Epoch 96/100\n",
      "178311/178311 [==============================] - 11s 60us/step - loss: 0.0625 - acc: 0.7459 - val_loss: 0.0629 - val_acc: 0.7452\n",
      "Epoch 97/100\n",
      "178311/178311 [==============================] - 11s 62us/step - loss: 0.0624 - acc: 0.7469 - val_loss: 0.0621 - val_acc: 0.7482\n",
      "Epoch 98/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0624 - acc: 0.7469 - val_loss: 0.0628 - val_acc: 0.7497\n",
      "Epoch 99/100\n",
      "178311/178311 [==============================] - 12s 66us/step - loss: 0.0624 - acc: 0.7471 - val_loss: 0.0625 - val_acc: 0.7426\n",
      "Epoch 100/100\n",
      "178311/178311 [==============================] - 10s 56us/step - loss: 0.0624 - acc: 0.7461 - val_loss: 0.0621 - val_acc: 0.7456\n",
      "28303/28303 [==============================] - 1s 27us/step\n",
      "\n",
      "Loss: 0.06190314461262459\n",
      "\n",
      "Accuracy: 74.79419142920756%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Cross-val fold:  10  is complete!\n",
      "#################################\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=10, shuffle=True, random_state=123)\n",
    "\n",
    "errors = []\n",
    "accuracies = []\n",
    "models = []\n",
    "f = 0\n",
    "\n",
    "for train, test in folds.split(train_data):\n",
    "    X_train, Y_train = train_data[train], encode_class[train]\n",
    "    X_test, Y_test = train_data[test], encode_class[test]\n",
    "    \n",
    "    print(\"Train shapes: \", X_train.shape, Y_train.shape)\n",
    "    print(\"Test shapes: \", X_test.shape, Y_test.shape)\n",
    "    \n",
    "    # further splitting the train set into train and validation sets\n",
    "    X_train_val, X_validation, Y_train_val, Y_validation = train_test_split(X_train, Y_train, test_size=0.3, random_state=123)\n",
    "    \n",
    "    # creating the nn architecture\n",
    "    # input layer - 9 features plus 1 bias\n",
    "    # hidden layer - 50 neurons\n",
    "    # output layer - 5 neurons for 5 class classifier\n",
    "    model = Sequential([Dense(10, input_dim=9, activation='relu'), Dense(50, activation='relu'), Dense(5)])\n",
    "\n",
    "    # compiling the nn using adam optimizer and mean squared error as loss function\n",
    "    model.compile(optimizer='adam',loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    #testing parameters\n",
    "    results = model.fit(X_train_val, Y_train_val, epochs=100, validation_data=(X_validation, Y_validation))\n",
    "    test_results = model.evaluate(X_test, Y_test)\n",
    "\n",
    "    print(\"\\nLoss: {}\".format(test_results[0]))\n",
    "    print(\"\\nAccuracy: {}%\".format(test_results[1]*100))\n",
    "    print(model.summary())\n",
    "    \n",
    "    f += 1\n",
    "    print(\"Cross-val fold: \", f, \" is complete!\")\n",
    "    print('#################################\\n\\n\\n')\n",
    "    \n",
    "    # collect the loss and accuracies\n",
    "    errors.append(test_results[0])\n",
    "    accuracies.append(test_results[1]*100)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b. Making Predictions with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GYROSCOPE', 'GRAVITY', 'ACCELEROMETER'}\n",
      "Test data files 'Type' feature is one hot encoded:  (75194, 3)\n",
      "Test data normalisation completed:  (75194, 4)\n",
      "Combined test data:  (75194, 9)\n"
     ]
    }
   ],
   "source": [
    "# moving onto the test data files...\n",
    "\n",
    "# encode the test data 'type' feature\n",
    "test_encode_type = one_hot(pd.DataFrame(pd_test, columns=cols[:-1]), 'Type', 0)\n",
    "print(\"Test data files 'Type' feature is one hot encoded: \", test_encode_type.shape)\n",
    "\n",
    "# normalize test data\n",
    "test_norm = std_norm(pd_test[:, 1:])\n",
    "print(\"Test data normalisation completed: \", test_norm.shape)\n",
    "\n",
    "# combine the test dataset\n",
    "zero = np.zeros(test_encode_type.shape[0]).reshape(-1, 1)\n",
    "test_data = np.concatenate((test_encode_type[:, 1].reshape(-1, 1), zero, test_encode_type[:, 2].reshape(-1, 1), zero,\n",
    "                            test_encode_type[:, 0].reshape(-1, 1), test_norm), axis=1)\n",
    "print(\"Combined test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors found in each of the trained models: \n",
      "[0.06086042340989122, 0.059165786544820256, 0.06585255669668914, 0.06298659515707691, 0.0612338049603078, 0.06532437463072135, 0.06320236865234295, 0.060013586100144414, 0.06417451622437426, 0.06190314461262459]\n",
      "Accuracy calculated in each of the trained models: \n",
      "[75.23318258903335, 75.87266817410966, 73.32532504239684, 74.667891464104, 75.14397767058017, 73.42331201670991, 74.56453379518834, 75.75522029503695, 74.11935130567326, 74.79419142920756]\n"
     ]
    }
   ],
   "source": [
    "# looking into model results\n",
    "print(\"Errors found in each of the trained models: \")\n",
    "print(errors)\n",
    "print(\"Accuracy calculated in each of the trained models: \")\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classes found in, model- 0  -  {0, 1, 2, 3, 4}\n",
      "The classes found in, model- 1  -  {0, 1, 2, 3}\n",
      "The classes found in, model- 2  -  {0, 1, 2, 3}\n",
      "The classes found in, model- 3  -  {0, 1, 3}\n",
      "The classes found in, model- 4  -  {0, 1, 2, 3, 4}\n",
      "The classes found in, model- 5  -  {0, 1, 2, 3, 4}\n",
      "The classes found in, model- 6  -  {0, 1, 3}\n",
      "The classes found in, model- 7  -  {0, 1, 2, 3}\n",
      "The classes found in, model- 8  -  {0, 1, 2, 3}\n",
      "The classes found in, model- 9  -  {1, 3}\n"
     ]
    }
   ],
   "source": [
    "# prediction and store final test results\n",
    "predicted = []\n",
    "\n",
    "# using each model to get results\n",
    "for mod in range(len(models)):\n",
    "    predict = models[mod].predict_classes(test_data)\n",
    "    print(\"The classes found in, model-\", mod, \" - \", set(predict))\n",
    "    predicted.append(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# testing model aggragation / ensemble learning\n",
    "\n",
    "final_class = np.zeros(test_data.shape[0]).reshape(test_data.shape[0], 1)\n",
    "\n",
    "for idx in range(len(test_data)):\n",
    "    sample_res = []\n",
    "\n",
    "    for res in predicted:\n",
    "        lab = res[idx]\n",
    "        sample_res.append(lab)\n",
    "        \n",
    "    # create an empty list to count number of occurences of a class\n",
    "    final = [0] * 5\n",
    "    \n",
    "    for i in sample_res:\n",
    "        final[i] += 1\n",
    "        \n",
    "    max_lab = max(final)\n",
    "    \n",
    "    pt = 0\n",
    "    for i in range(len(final)):\n",
    "        if final[i] == max_lab:\n",
    "            pt = i\n",
    "            break\n",
    "            \n",
    "    final_class[idx] = pt\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes obtained as a result of the 'best model':  {0, 1, 2, 3, 4}\n",
      "Label of each class:  {'data-ear-', 'data-pocket-', 'data-hand-landscape-', 'data-hand-swinging-', 'data-hand-portrait-'}\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "# interestingly not all models return each class \n",
    "# problem in training where each training data does not contain samples of certain classes\n",
    "# hence we will choose a model that has good accuracy and includes all the classes\n",
    "# model 0 is choosen\n",
    "\n",
    "print(\"Classes obtained as a result of the 'best model': \", set(predicted[0]))\n",
    "print(\"Label of each class: \", set(unique_id))\n",
    "\n",
    "class_names = list(set(unique_id))\n",
    "predicted_names = []\n",
    "for i in predicted[0]:\n",
    "    name = class_names[i]\n",
    "    predicted_names.append(name)\n",
    "    \n",
    "encoded_result = one_hot(pd.DataFrame(predicted[0], columns=['class_id']), 'class_id', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75194, 1), (75194, 5), (75194, 5))"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the final new dataframe of results\n",
    "pd_classes = pd.DataFrame(predicted_names, columns=['class_id'])\n",
    "pd_encoded = pd.DataFrame(encoded_result, columns=class_names)\n",
    "pd_tested = pd.DataFrame(pd_test, columns=cols[:-1])\n",
    "\n",
    "pd_classes.shape, pd_encoded.shape, pd_tested.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result dataframe has been formed!  (75194, 11)\n",
      "Final results have been stored in 'test_results.csv'!!\n"
     ]
    }
   ],
   "source": [
    "# setting up the final dataframe\n",
    "pd_result = pd.concat([pd_tested, pd_encoded, pd_classes], axis=1)\n",
    "print(\"Final result dataframe has been formed! \",pd_result.shape)\n",
    "\n",
    "# store the final dataframe as a csv file\n",
    "pd_result.to_csv('test_results.csv', index=False)\n",
    "print(\"Final results have been stored in 'test_results.csv'!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
